{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:\n",
      "`load_boston` has been removed from scikit-learn since version 1.2.\n",
      "\n",
      "The Boston housing prices dataset has an ethical problem: as\n",
      "investigated in [1], the authors of this dataset engineered a\n",
      "non-invertible variable \"B\" assuming that racial self-segregation had a\n",
      "positive impact on house prices [2]. Furthermore the goal of the\n",
      "research that led to the creation of this dataset was to study the\n",
      "impact of air quality but it did not give adequate demonstration of the\n",
      "validity of this assumption.\n",
      "\n",
      "The scikit-learn maintainers therefore strongly discourage the use of\n",
      "this dataset unless the purpose of the code is to study and educate\n",
      "about ethical issues in data science and machine learning.\n",
      "\n",
      "In this special case, you can fetch the dataset from the original\n",
      "source::\n",
      "\n",
      "    import pandas as pd\n",
      "    import numpy as np\n",
      "\n",
      "    data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n",
      "    raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n",
      "    data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n",
      "    target = raw_df.values[1::2, 2]\n",
      "\n",
      "Alternative datasets include the California housing dataset and the\n",
      "Ames housing dataset. You can load the datasets as follows::\n",
      "\n",
      "    from sklearn.datasets import fetch_california_housing\n",
      "    housing = fetch_california_housing()\n",
      "\n",
      "for the California housing dataset and::\n",
      "\n",
      "    from sklearn.datasets import fetch_openml\n",
      "    housing = fetch_openml(name=\"house_prices\", as_frame=True)\n",
      "\n",
      "for the Ames housing dataset.\n",
      "\n",
      "[1] M Carlisle.\n",
      "\"Racist data destruction?\"\n",
      "<https://medium.com/@docintangible/racist-data-destruction-113e3eff54a8>\n",
      "\n",
      "[2] Harrison Jr, David, and Daniel L. Rubinfeld.\n",
      "\"Hedonic housing prices and the demand for clean air.\"\n",
      "Journal of environmental economics and management 5.1 (1978): 81-102.\n",
      "<https://www.researchgate.net/publication/4974606_Hedonic_housing_prices_and_the_demand_for_clean_air>\n",
      ": LawSchoolGPADataset will be unavailable. To install, run:\n",
      "pip install 'aif360[LawSchoolGPA]'\n",
      "WARNING:root:No module named 'rpy2': FairAdapt will be unavailable. To install, run:\n",
      "pip install 'aif360[FairAdapt]'\n"
     ]
    }
   ],
   "source": [
    "from aif360.metrics import BinaryLabelDatasetMetric\n",
    "from aif360.metrics import ClassificationMetric\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "\n",
    "from aif360.algorithms.preprocessing.optim_preproc_helpers.data_preproc_functions \\\n",
    "import load_preproc_data_adult #, load_preproc_data_german, load_preproc_data_compas\n",
    "\n",
    "from aif360.algorithms.preprocessing.optim_preproc_helpers.distortion_functions\\\n",
    "            import get_distortion_adult #, get_distortion_german, get_distortion_compas\n",
    "\n",
    "# Bias mitigation preproc-techniques\n",
    "\n",
    "from aif360.sklearn.preprocessing import Reweighing\n",
    "\n",
    "from aif360.algorithms.preprocessing import Reweighing\n",
    "from aif360.algorithms.preprocessing.optim_preproc import OptimPreproc\n",
    "from aif360.algorithms.preprocessing.optim_preproc_helpers.opt_tools import OptTools\n",
    "from aif360.algorithms.preprocessing import DisparateImpactRemover\n",
    "from aif360.algorithms.preprocessing.lfr import LFR\n",
    "\n",
    "from aif360.algorithms.preprocessing.optim_preproc_helpers.data_preproc_functions\\\n",
    "import load_preproc_data_adult\n",
    "\n",
    "# from aif360.algorithms.preprocessing.optim_preproc_helpers import data_preproc_functions\n",
    "\n",
    "from aif360.algorithms.preprocessing.optim_preproc_helpers.opt_tools import OptTools\n",
    "\n",
    "#from aif360.algorithms.inprocessing import PrejudiceRemover\n",
    "\n",
    "# from aif360.datasets.lime_encoder import LimeEncoder\n",
    "# import lime\n",
    "# from lime.lime_tabular import LimeTabularExplainer\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from aif360.datasets import BinaryLabelDataset\n",
    "from aif360.datasets import StructuredDataset\n",
    "from aif360.datasets import AdultDataset\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "# inprocessing\n",
    "# from aif360.algorithms.inprocessing.adversarial_debiasing import AdversarialDebiasing\n",
    "from aif360.sklearn.inprocessing import AdversarialDebiasing\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "# postprocessing\n",
    "from aif360.algorithms.postprocessing.reject_option_classification\\\n",
    "        import RejectOptionClassification\n",
    "# Odds equalizing post-processing algorithm\n",
    "from aif360.algorithms.postprocessing.calibrated_eq_odds_postprocessing import CalibratedEqOddsPostprocessing\n",
    "# from aif360.algorithms.postprocessing.eq_odds_postprocessing import EqOddsPostprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['race',\n",
       " 'sex',\n",
       " 'Age (decade)=10',\n",
       " 'Age (decade)=20',\n",
       " 'Age (decade)=30',\n",
       " 'Age (decade)=40',\n",
       " 'Age (decade)=50',\n",
       " 'Age (decade)=60',\n",
       " 'Age (decade)=>=70',\n",
       " 'Education Years=6',\n",
       " 'Education Years=7',\n",
       " 'Education Years=8',\n",
       " 'Education Years=9',\n",
       " 'Education Years=10',\n",
       " 'Education Years=11',\n",
       " 'Education Years=12',\n",
       " 'Education Years=<6',\n",
       " 'Education Years=>12']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "privileged_groups = [{'sex': 1}]\n",
    "unprivileged_groups = [{'sex': 0}]\n",
    "dataset_orig = load_preproc_data_adult(['sex'])\n",
    "dataset_orig.feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler= StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_orig_train, dataset_orig_vt = dataset_orig.split([0.7], shuffle=True)\n",
    "dataset_orig_valid, dataset_orig_test = dataset_orig_vt.split([0.5], shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "protected = 'sex'\n",
    "# dataset_orig_train.features = scaler.fit_transform(dataset_orig_train.features)\n",
    "# dataset_orig_valid.features = scaler.fit_transform(dataset_orig_valid.features)\n",
    "# dataset_orig_test.features = scaler.fit_transform(dataset_orig_test.features)\n",
    "\n",
    "index = dataset_orig_train.feature_names.index(protected)\n",
    "\n",
    "# di = DisparateImpactRemover(repair_level=1)\n",
    "# data = di.fit_transform(dataset_orig_train)\n",
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "debug = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred_preproc_fit_model(train, valid, test):\n",
    "\n",
    "    # fitting on training data\n",
    "    scale_orig = StandardScaler()\n",
    "    x_train = scale_orig.fit_transform(train.features)\n",
    "    y_train = train.labels.ravel()\n",
    "    # w_train = train.instance_weights.ravel()\n",
    "\n",
    "    if debug==1:\n",
    "        print(\"Scaler OK\")\n",
    "\n",
    "    lmod = LogisticRegression()\n",
    "    lmod.fit(x_train, y_train,\n",
    "         sample_weight=train.instance_weights)\n",
    "    \n",
    "    if debug==1:\n",
    "        print(\"Model fit OK\")\n",
    "    \n",
    "\n",
    "    # prediction on training data\n",
    "    y_train_pred = lmod.predict(x_train)\n",
    "\n",
    "    if debug==1:\n",
    "        print(\"y pred OK\")\n",
    "\n",
    "    # positive class index\n",
    "    pos_ind = np.where(lmod.classes_ == train.favorable_label)[0][0]\n",
    "\n",
    "    if debug==1:\n",
    "        print(\"pos_ind OK\")\n",
    "\n",
    "    train_pred = train.copy(deepcopy=True)\n",
    "    # train_pred_labels = y_train_pred\n",
    "\n",
    "\n",
    "    # prediction on validation and test data and score calculation\n",
    "    valid_pred = valid.copy(deepcopy=True)\n",
    "    X_valid = scale_orig.transform(valid_pred.features)\n",
    "    # y_valid = valid_pred.labels\n",
    "    valid_pred.scores = lmod.predict_proba(X_valid)[:,pos_ind].reshape(-1,1)\n",
    "\n",
    "    if debug==1:\n",
    "        print(\"valid OK\")\n",
    "\n",
    "    test_pred = test.copy(deepcopy=True)\n",
    "    X_test = scale_orig.transform(test_pred.features)\n",
    "    # y_test = test_pred.labels\n",
    "    test_pred.scores = lmod.predict_proba(X_test)[:,pos_ind].reshape(-1,1)\n",
    "\n",
    "    if debug==1:\n",
    "        print(\"test OK\")\n",
    "\n",
    "    num_thresh = 100\n",
    "    ba_arr = np.zeros(num_thresh)\n",
    "    class_thresh_arr = np.linspace(0.01, 0.99, num_thresh)\n",
    "\n",
    "    if debug==1:\n",
    "        print(\"def thresh OK\")\n",
    "\n",
    "\n",
    "\n",
    "    # balanced accuracy, threshold\n",
    "    for idx, class_thresh in enumerate(class_thresh_arr):\n",
    "\n",
    "        fav_inds = valid_pred.scores > class_thresh\n",
    "        valid_pred.labels[fav_inds] = valid_pred.favorable_label\n",
    "        valid_pred.labels[~fav_inds] = valid_pred.unfavorable_label\n",
    "\n",
    "        if debug==1:\n",
    "            print(\"best thresh sel OK\")\n",
    "\n",
    "        classified_metric_valid = ClassificationMetric(valid,\n",
    "                                                 valid_pred,\n",
    "                                                 unprivileged_groups=unprivileged_groups,\n",
    "                                                 privileged_groups=privileged_groups)\n",
    "        if debug==1:\n",
    "            print(\"class metric valid OK\")\n",
    "\n",
    "\n",
    "        ba_arr[idx] = 0.5*(classified_metric_valid.true_positive_rate()\\\n",
    "                       +classified_metric_valid.true_negative_rate())\n",
    "\n",
    "    best_ind = np.where(ba_arr == np.max(ba_arr))[0][0]\n",
    "    best_class_thresh = class_thresh_arr[best_ind]\n",
    "\n",
    "    if debug==1:\n",
    "        print(\"best class thresh OK\")\n",
    "\n",
    "    \n",
    "\n",
    "    return train_pred, valid_pred, test_pred, best_class_thresh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reweighing(datas, unprivileged_groups=unprivileged_groups,privileged_groups=privileged_groups):\n",
    "\n",
    "#     scale_orig = StandardScaler()\n",
    "#     data= scale_orig.fit_transform(data)\n",
    "    dataset_transf = []\n",
    "    for data in datas:\n",
    "      RW = Reweighing(unprivileged_groups=unprivileged_groups,\n",
    "                    privileged_groups=privileged_groups)\n",
    "\n",
    "      RW.fit(data)\n",
    "\n",
    "      dataset_transf.append(RW.transform(data))\n",
    "\n",
    "    return dataset_transf\n",
    "\n",
    "\n",
    "def disparate_impact_remover(datas, level=1):\n",
    "\n",
    "    # scale_orig = StandardScaler()\n",
    "    # data.features= scale_orig.fit_transform(data.features)\n",
    "\n",
    "    dataset_transf = []\n",
    "    for data in datas:\n",
    "      di = DisparateImpactRemover(repair_level=level)\n",
    "      dataset_transf.append( di.fit_transform(data))\n",
    "\n",
    "    return dataset_transf\n",
    "\n",
    "\n",
    "def lfr(datas, unprivileged_groups=unprivileged_groups,privileged_groups=privileged_groups):\n",
    "\n",
    "    # scale_orig = StandardScaler()\n",
    "    # data.features = scale_orig.fit_transform(data.features)\n",
    "\n",
    "    dataset_transf = []\n",
    "    for data in datas:\n",
    "      TR = LFR(unprivileged_groups=unprivileged_groups,\n",
    "          privileged_groups=privileged_groups,\n",
    "          k=10, Ax=0.1, Ay=1.0, Az=2.0,\n",
    "          verbose=1\n",
    "          )\n",
    "      TR = TR.fit(data, maxiter=5000, maxfun=5000)\n",
    "      dataset_transf.append(TR.transform(data))\n",
    "\n",
    "    return dataset_transf\n",
    "\n",
    "\n",
    "\n",
    "def optim_preproc(datas):\n",
    "\n",
    "  dataset_transf = []\n",
    "  for data in datas:\n",
    "    optim_options = {\n",
    "      \"distortion_fun\": get_distortion_adult,\n",
    "      \"epsilon\": 0.05,\n",
    "      \"clist\": [0.99, 1.99, 2.99],\n",
    "      \"dlist\": [.1, 0.05, 0]\n",
    "    }\n",
    "        \n",
    "    OP = OptimPreproc(OptTools, optim_options)\n",
    "    OP = OP.fit(data)\n",
    "    temp = OP.transform(data, transform_Y=True)\n",
    "    dataset_transf.append(data.align_datasets(temp))\n",
    "\n",
    "  return dataset_transf\n",
    "\n",
    "\n",
    "# # From example \n",
    "# def adversial_debiasing(datas):\n",
    "#   dataset_transf = []\n",
    "#   tf.compat.v1.disable_eager_execution()\n",
    "#   tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "  \n",
    "#   for data in datas:\n",
    "#     if np.any(data.features==None):\n",
    "#       continue\n",
    "#     # sess = tf.Session()\n",
    "#     with tf.compat.v1.variable_scope('debiased_classifier', reuse=tf.compat.v1.AUTO_REUSE):\n",
    "      \n",
    "#       with tf.compat.v1.Session() as sess:\n",
    "#         debiased_model = AdversarialDebiasing(privileged_groups = privileged_groups,\n",
    "#                               unprivileged_groups = unprivileged_groups,\n",
    "#                               scope_name='debiased_classifier',\n",
    "#                               debias=True,\n",
    "#                               sess=sess)\n",
    "#         sess.run(tf.compat.v1.global_variables_initializer())  # Initialize global variables\n",
    "#         sess.run(tf.compat.v1.local_variables_initializer())\n",
    "#         debiased_model.fit(data)\n",
    "\n",
    "\n",
    "#         dataset_transf.append(debiased_model.predict(data))\n",
    "\n",
    "#     sess.close()\n",
    "\n",
    "#   return dataset_transf\n",
    "\n",
    "\n",
    "\n",
    "# def adversial_debiasing(datas):\n",
    "#   dataset_transf = []\n",
    "  \n",
    "#   for data in datas:\n",
    "#     if np.any(data.features==None):\n",
    "#       continue\n",
    "#     # sess = tf.Session()\n",
    "#     # with tf.compat.v1.variable_scope('debiased_classifier', reuse=tf.compat.v1.AUTO_REUSE):\n",
    "#     tf.compat.v1.disable_eager_execution()\n",
    "#       # with tf.compat.v1.Session() as sess:\n",
    "#     debiased_model = AdversarialDebiasing(\n",
    "#                                           prot_attr=[data.protected_attributes], \n",
    "#                                           # scope_name='debiased_classifier', \n",
    "#                                           # adversary_loss_weight=0.1, \n",
    "#                                           # num_epochs=50, batch_size=128, \n",
    "#                                           # classifier_num_hidden_units=200, \n",
    "#                                           debias=True, verbose=False, random_state=None)\n",
    "#         # sess.run(tf.compat.v1.global_variables_initializer())  # Initialize global variables\n",
    "#         # sess.run(tf.compat.v1.local_variables_initializer())\n",
    "    \n",
    "    \n",
    "#     # print(type(data))\n",
    "#     # df = StructuredDataset.convert_to_dataframe(data)[0]\n",
    "#     # # print(df)\n",
    "\n",
    "#     # x = df.drop(data.label_names, axis=1)\n",
    "#     # y = df[data.label_names[0]]\n",
    "\n",
    "#     # print(y)\n",
    "   \n",
    "#     # data = StructuredDataset(df, data.label_names, data.protected_attributes, data.instance_weights)\n",
    "\n",
    "#     debiased_model.fit(data.features, data.labels.ravel())\n",
    "\n",
    "\n",
    "#     dataset_transf.append(debiased_model.predict(data))\n",
    "\n",
    "#     # sess.close()\n",
    "\n",
    "#   return dataset_transf\n",
    "\n",
    "\n",
    "def in1(ls):      # adversial debiasing\n",
    "  return ls\n",
    "\n",
    "def in2(ls):\n",
    "  return ls\n",
    "\n",
    "def in3(ls):\n",
    "    return ls\n",
    "\n",
    "def RejOpt(datas, datapreds):\n",
    "  dataset_transf = []\n",
    "  for data, data_pred in zip(datas, datapreds):\n",
    "    ROC = RejectOptionClassification(unprivileged_groups=unprivileged_groups, \n",
    "                                  privileged_groups=privileged_groups, \n",
    "                                  low_class_thresh=0.01, high_class_thresh=0.99,\n",
    "                                    num_class_thresh=100, num_ROC_margin=50,\n",
    "                                    metric_name=\"Statistical parity difference\",\n",
    "                                    metric_ub=0.05, metric_lb=-0.05)\n",
    "    ROC = ROC.fit(data, data_pred)\n",
    "    dataset_transf.append(ROC.predict(data))\n",
    "\n",
    "    # dataset_transf.append(ROC.fit_transform(data, pred))\n",
    "\n",
    "  return dataset_transf\n",
    "\n",
    "def Cal_EqOdds(datas, datapreds):\n",
    "  dataset_transf = []\n",
    "  for data, data_pred in zip(datas,datapreds):\n",
    "    cpp = CalibratedEqOddsPostprocessing(privileged_groups = privileged_groups,\n",
    "                                     unprivileged_groups = unprivileged_groups,\n",
    "                                     cost_constraint='weighted'\n",
    "                                     )\n",
    "    cpp = cpp.fit(data, data_pred)\n",
    "    dataset_transf.append(cpp.predict(data_pred))\n",
    "  \n",
    "  return dataset_transf\n",
    "\n",
    "''' \n",
    "\n",
    "        From Eqodds postprocessing library source file:\n",
    "\n",
    "        metric = ClassificationMetric(dataset_true, dataset_pred,\n",
    "            unprivileged_groups=self.unprivileged_groups,\n",
    "            privileged_groups=self.privileged_groups)\n",
    "\n",
    "        # compute basic statistics\n",
    "        sbr = metric.base_rate(privileged=True)\n",
    "        obr = metric.base_rate(privileged=False)\n",
    "\n",
    "        fpr0 = metric.false_positive_rate(privileged=True)\n",
    "        fpr1 = metric.false_positive_rate(privileged=False)\n",
    "        fnr0 = metric.false_negative_rate(privileged=True)\n",
    "        fnr1 = metric.false_negative_rate(privileged=False)\n",
    "        tpr0 = metric.true_positive_rate(privileged=True)\n",
    "        tpr1 = metric.true_positive_rate(privileged=False)\n",
    "        tnr0 = metric.true_negative_rate(privileged=True)\n",
    "        tnr1 = metric.true_negative_rate(privileged=False)\n",
    "\n",
    "        # linear program has 4 decision variables:\n",
    "        # [Pr[label_tilde = 1 | label_hat = 1, protected_attributes = 0];\n",
    "        #  Pr[label_tilde = 1 | label_hat = 0, protected_attributes = 0];\n",
    "        #  Pr[label_tilde = 1 | label_hat = 1, protected_attributes = 1];\n",
    "        #  Pr[label_tilde = 1 | label_hat = 0, protected_attributes = 1]]\n",
    "        # Coefficients of the linear objective function to be minimized.\n",
    "        c = np.array([fpr0 - tpr0, tnr0 - fnr0, fpr1 - tpr1, tnr1 - fnr1])\n",
    "        \n",
    "\n",
    "        Error on executing below function\n",
    "        ValueError: Invalid input for linprog: c must not contain values inf, nan, or None\n",
    "        '''\n",
    "\n",
    "# def EqOdds(datas, datapreds):\n",
    "#   dataset_transf = []\n",
    "#   for data, data_pred in zip(datas,datapreds):\n",
    "#     epp = EqOddsPostprocessing(unprivileged_groups=unprivileged_groups, privileged_groups=privileged_groups, seed=None)\n",
    "#     epp = epp.fit(data, data_pred)\n",
    "#     dataset_transf.append(epp.predict(data_pred))\n",
    "\n",
    "#   return dataset_transf\n",
    "\n",
    "\n",
    "def EqOdds(datas, datapreds):\n",
    "  return datapreds\n",
    "\n",
    "def evaluate_performance(post_processed_data, labels):\n",
    "  pass\n",
    "\n",
    "# empty function for original data\n",
    "def pre_n_in_no_fn(ls):\n",
    "  # train = ls[0]\n",
    "  # valid = ls[1]\n",
    "  # test = ls[2]\n",
    "  # return train,valid,test\n",
    "  return ls\n",
    "\n",
    "def post_no_fn(ls, ls2):\n",
    "  # train = ls[0]\n",
    "  # valid = ls[1]\n",
    "  # test = ls[2]\n",
    "  # return train,valid,test\n",
    "  return ls2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retest rejopt function with fit_transform\n",
    "\n",
    "if debug == 1:\n",
    "    train_pred, valid_pred, test_pred, best_class_thresh = pred_preproc_fit_model(dataset_orig_train, dataset_orig_valid, dataset_orig_test)\n",
    "    ls = EqOdds([dataset_orig_train, dataset_orig_valid, dataset_orig_test],[train_pred, valid_pred, test_pred])\n",
    "    print(ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if debug == 1:\n",
    "#     ls = adversial_debiasing([dataset_orig_train, dataset_orig_valid, dataset_orig_test])\n",
    "#     ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., ..., 0., 0., 1.],\n",
       "       [0., 1., 0., ..., 0., 0., 1.],\n",
       "       [1., 1., 0., ..., 0., 0., 1.],\n",
       "       ...,\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 1., 0., ..., 0., 0., 1.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_orig_train.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_metric(test, split_data_pred, best_class_thresh):\n",
    "        fav_inds = split_data_pred.scores > best_class_thresh\n",
    "        split_data_pred.labels[fav_inds] = split_data_pred.favorable_label\n",
    "        split_data_pred.labels[~fav_inds] = split_data_pred.unfavorable_label\n",
    "\n",
    "        \n",
    "\n",
    "        metric_obj = ClassificationMetric(test, split_data_pred,\n",
    "                                        unprivileged_groups, privileged_groups)\n",
    "        \n",
    "        metric = []\n",
    "\n",
    "        metric.append(abs(metric_obj.mean_difference()))\n",
    "        metric.append(abs(1-metric_obj.disparate_impact()))\n",
    "        # metric.append(abs(metric_obj.equal_opportunity_difference()))\n",
    "        # metric.append(abs(metric_obj.average_odds_difference()))\n",
    "        metric.append(abs(metric_obj.theil_index()))\n",
    "\n",
    "        avg = 0\n",
    "        for m in metric:\n",
    "            avg += m\n",
    "\n",
    "        avg = avg/len(metric)\n",
    "\n",
    "        return avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "from itertools import product\n",
    "\n",
    "def test_preprocessing_combinations(train,valid,test):\n",
    "    # Define the preprocessing algorithms\n",
    "    preprocessing_algos = {\n",
    "        'pre1': reweighing,\n",
    "        'pre2': disparate_impact_remover,\n",
    "        'pre3': lfr,\n",
    "        'pre4': optim_preproc,                           #### try later\n",
    "        'pre_none': pre_n_in_no_fn\n",
    "    }\n",
    "\n",
    "    # Define the in-processing algorithms\n",
    "    in_processing_algos = {\n",
    "        'in1': in1,\n",
    "        'in2': in2,\n",
    "        'in3': in3,\n",
    "        'in_none': pre_n_in_no_fn\n",
    "    }\n",
    "\n",
    "    # Define the post-processing algorithms\n",
    "    post_processing_algos = {\n",
    "        'post1': RejOpt,\n",
    "        'post2': Cal_EqOdds,\n",
    "        'post3': EqOdds,\n",
    "        'post_none': post_no_fn\n",
    "    }\n",
    "\n",
    "    # Store the scores for each combination\n",
    "    combination_scores = {}\n",
    "\n",
    "    # Generate all possible combinations of preprocessing, in-processing, and post-processing algorithms\n",
    "    all_combinations = product(preprocessing_algos.keys(), \n",
    "                               #in_processing_algos.keys(), \n",
    "                               post_processing_algos.keys())\n",
    "\n",
    "    preproc_data = {}\n",
    "\n",
    "    # Test each combination and store the scores\n",
    "    for combination in all_combinations:\n",
    "        print(combination)\n",
    "        # pre_algo, in_algo, post_algo = combination\n",
    "        pre_algo, post_algo = combination\n",
    "\n",
    "        # train = og_train.copy(deepcopy=True)\n",
    "        # valid = og_valid.copy(deepcopy=True)\n",
    "        # test = og_test.copy(deepcopy=True)\n",
    "\n",
    "\n",
    "        try:\n",
    "        #   preprocessed_data = preproc_data[pre_algo]\n",
    "            ls = preproc_data[pre_algo]\n",
    "            # pre_train = ls[0]\n",
    "            # pre_valid = ls[1]\n",
    "            # pre_test = ls[2]\n",
    "\n",
    "            # optimizing space comp\n",
    "            trf_train = ls[0]\n",
    "            trf_valid = ls[1]\n",
    "            trf_test = ls[2]\n",
    "\n",
    "        except:\n",
    "          # Apply the preprocessing algorithm\n",
    "          # preprocessed_data = preprocessing_algos[pre_algo](train, valid, test)\n",
    "            [trf_train, trf_valid, trf_test] = preprocessing_algos[pre_algo]([train, valid, test])\n",
    "            # preproc_data[pre_algo] = preprocessed_data\n",
    "            preproc_data[pre_algo] = [trf_train, trf_valid, trf_test]\n",
    "        \n",
    "        if debug ==1:\n",
    "            # if og_train == train:\n",
    "            #     print(\"train not modif\")\n",
    "            print(trf_train, trf_valid, trf_test)\n",
    "            # check_pt = pre_train.copy(deepcopy=True)\n",
    "            # check_pv = pre_valid.copy(deepcopy=True)\n",
    "            # check_ptt = pre_test.copy(deepcopy=True)\n",
    "\n",
    "        \n",
    "\n",
    "        train_pred, valid_pred, test_pred, best_class_thresh = pred_preproc_fit_model(trf_train, trf_valid, trf_test)\n",
    "\n",
    "        if debug==1:\n",
    "            # if check_pt == pre_train:\n",
    "                # print(\"pre train not modif\")\n",
    "            print(\"pred proc OK\")\n",
    "\n",
    "            # print(train_pred, valid_pred, test_pred, best_class_thresh)\n",
    "            # print(train_pred)\n",
    "            # print(valid_pred)\n",
    "            print(\"test: \\n\", test)\n",
    "            print(\"test_pred: \\n\", test_pred)\n",
    "            # print(best_class_thresh)\n",
    "        \n",
    "        # Apply the in-processing algorithm\n",
    "        # [trf_train, trf_valid, trf_test] = in_processing_algos[in_algo]([trf_train, trf_valid, trf_test])\n",
    "\n",
    "        # Apply the post-processing algorithm\n",
    "        [trf_train, trf_valid, trf_test] = post_processing_algos[post_algo]([trf_train, trf_valid, trf_test], [train_pred, valid_pred, test_pred])\n",
    "\n",
    "        # Evaluate the performance and store the score\n",
    "        # score = evaluate_performance(post_processed_data, labels)\n",
    "        \n",
    "        # # removing labels from test pred for metric function\n",
    "        # temp_label = test_pred.labels\n",
    "        # del test_pred.labels\n",
    "        # # test_pred.labels = None\n",
    "        # test2 = test_pred.copy(deepcopy=True)\n",
    "        # test_pred.labels = temp_label\n",
    "\n",
    "        # # removing labels try 2\n",
    "        # temp_label = test_pred.labels\n",
    "        # del test_pred.features['labels']  # Remove the 'labels' column from the features\n",
    "        # test2 = test_pred.copy(deepcopy=True)\n",
    "        # # Assign the original labels back to the modified dataset\n",
    "        # test_pred.features['labels'] = temp_label\n",
    "\n",
    "        # test_pred = BinaryLabelDataset(\n",
    "        #     favorable_label=1,\n",
    "        #     unfavorable_label=0,\n",
    "        #     df=test_pred,\n",
    "        #     label_names = test_pred.labels,\n",
    "        #     protected_attribute_names = ['sex']\n",
    "        # )\n",
    "\n",
    "        # split_data_pred = BinaryLabelDataset(\n",
    "        #     favorable_label=1,\n",
    "        #     unfavorable_label=0,\n",
    "        #     df=split_data_pred,\n",
    "        #     label_names = split_data_pred.labels,\n",
    "        #     protected_attribute_names = ['sex']\n",
    "        # )\n",
    "\n",
    "\n",
    "        score = cal_metric(trf_test, test_pred, best_class_thresh)  \n",
    "        combination_scores[combination] = score\n",
    "        \n",
    "\n",
    "    # Select the combination with the best score --> 0\n",
    "    best_combination = min(combination_scores, key=combination_scores.get)\n",
    "    best_score = combination_scores[best_combination]\n",
    "\n",
    "    return best_combination, best_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('pre1', 'post1')\n",
      "('pre1', 'post2')\n",
      "('pre1', 'post3')\n",
      "('pre1', 'post_none')\n",
      "('pre2', 'post1')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\singh\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\aif360\\algorithms\\postprocessing\\reject_option_classification.py:160: UserWarning: Unable to satisy fairness constraints\n",
      "  warn(\"Unable to satisy fairness constraints\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('pre2', 'post2')\n",
      "('pre2', 'post3')\n",
      "('pre2', 'post_none')\n",
      "('pre3', 'post1')\n",
      "step: 0, loss: 0.8636315014491124, L_x: 0.49642392851890066,  L_y: 0.7931940303732408,  L_z: 0.010397539111990742\n",
      "step: 250, loss: 0.7029605642414505, L_x: 0.49427718898149353,  L_y: 0.6333684704199326,  L_z: 0.010082187461684282\n",
      "step: 500, loss: 0.6243306271728277, L_x: 0.4882211857772604,  L_y: 0.558106201952459,  L_z: 0.008701153321321358\n",
      "step: 750, loss: 0.6163753111118245, L_x: 0.48396111789932506,  L_y: 0.5536038401466898,  L_z: 0.007187679587601102\n",
      "step: 1000, loss: 0.6000747383908153, L_x: 0.45804504459421524,  L_y: 0.5434536323011926,  L_z: 0.005408300815100571\n",
      "step: 1250, loss: 0.5898288482950883, L_x: 0.4309105060096078,  L_y: 0.5351520294232156,  L_z: 0.00579288413545598\n",
      "step: 1500, loss: 0.577620337038947, L_x: 0.39823017631517976,  L_y: 0.5279737863462755,  L_z: 0.004911766530576815\n",
      "step: 1750, loss: 0.5460172340162424, L_x: 0.28503508611712725,  L_y: 0.49824343507660723,  L_z: 0.00963514516396121\n",
      "step: 2000, loss: 0.5396540419949878, L_x: 0.2655194711147819,  L_y: 0.4925583717286879,  L_z: 0.010271861577410859\n",
      "step: 2250, loss: 0.5293741727378511, L_x: 0.2171190619112956,  L_y: 0.47925019573041794,  L_z: 0.014206035408151815\n",
      "step: 2500, loss: 0.516161102638745, L_x: 0.204852174550222,  L_y: 0.4756652958226511,  L_z: 0.010005294680535858\n",
      "step: 2750, loss: 0.5114548842522512, L_x: 0.20123199506330663,  L_y: 0.472296061875611,  L_z: 0.009517811435154747\n",
      "step: 3000, loss: 0.5289910393915919, L_x: 0.20558316464958404,  L_y: 0.4889225758233987,  L_z: 0.00975507355161739\n",
      "step: 3250, loss: 0.5083650512720529, L_x: 0.19879946491082023,  L_y: 0.47133895065088394,  L_z: 0.008573077065043453\n",
      "step: 3500, loss: 0.507682567999977, L_x: 0.19802619627260581,  L_y: 0.47135384456888163,  L_z: 0.00826305190191738\n",
      "step: 3750, loss: 0.5068948862329027, L_x: 0.19615494294792585,  L_y: 0.472160731749635,  L_z: 0.007559330094237556\n",
      "step: 4000, loss: 0.5101550589513346, L_x: 0.19502343609945727,  L_y: 0.4763745407898196,  L_z: 0.007139087275784629\n",
      "step: 4250, loss: 0.5071083813009742, L_x: 0.19523026227876566,  L_y: 0.4753064146181412,  L_z: 0.006139470227478243\n",
      "step: 4500, loss: 0.5053301645844622, L_x: 0.19556939707412369,  L_y: 0.47389737174638213,  L_z: 0.005937926565333891\n",
      "step: 4750, loss: 0.5044213774887176, L_x: 0.19606474266044993,  L_y: 0.4733445453865239,  L_z: 0.005735178918074369\n",
      "step: 5000, loss: 0.5025900194534796, L_x: 0.19377177574394033,  L_y: 0.4731456058980951,  L_z: 0.005033617990495206\n",
      "step: 0, loss: 0.6885283662513907, L_x: 0.43766876704223756,  L_y: 0.634036560991066,  L_z: 0.005362464278050501\n",
      "step: 250, loss: 0.6306805089524437, L_x: 0.4348231936911111,  L_y: 0.5773573921203389,  L_z: 0.004920398731496874\n",
      "step: 500, loss: 0.5913334877422434, L_x: 0.4260060589207156,  L_y: 0.539919092640335,  L_z: 0.004406894604918372\n",
      "step: 750, loss: 0.5864219121864311, L_x: 0.42014337340896124,  L_y: 0.53784811724201,  L_z: 0.0032797288017625222\n",
      "step: 1000, loss: 0.5813061861287476, L_x: 0.40561230401744885,  L_y: 0.5348518071103839,  L_z: 0.0029465743083094374\n",
      "step: 1250, loss: 0.5762244261974195, L_x: 0.39374942932936957,  L_y: 0.5318894939223425,  L_z: 0.002479994671070025\n",
      "step: 1500, loss: 0.5730287276276818, L_x: 0.38280015403724177,  L_y: 0.5301859208065471,  L_z: 0.0022813957087052467\n",
      "step: 1750, loss: 0.5653225776228032, L_x: 0.30999050789928473,  L_y: 0.5214795597210492,  L_z: 0.0064219835559127785\n",
      "step: 2000, loss: 0.5578708867831947, L_x: 0.29222616545771923,  L_y: 0.5181681264405373,  L_z: 0.005240071898442719\n",
      "step: 2250, loss: 0.5456252119427021, L_x: 0.25611568196058354,  L_y: 0.5127757285143868,  L_z: 0.0036189576161284828\n",
      "step: 2500, loss: 0.536766368990447, L_x: 0.23234030093792626,  L_y: 0.5008307784153484,  L_z: 0.0063507802406529935\n",
      "step: 2750, loss: 0.5286272608368282, L_x: 0.21766391070833507,  L_y: 0.49591109666921024,  L_z: 0.005474886548392219\n",
      "step: 3000, loss: 0.5233270204650174, L_x: 0.21474412809814072,  L_y: 0.4921382345261949,  L_z: 0.0048571865645042215\n",
      "step: 3250, loss: 0.5151845501872252, L_x: 0.20426744244580536,  L_y: 0.4871562988941166,  L_z: 0.0038007535242640146\n",
      "step: 3500, loss: 0.5118792808162153, L_x: 0.20030284957529834,  L_y: 0.48555155276445644,  L_z: 0.003148721547114478\n",
      "step: 3750, loss: 0.5162800463395871, L_x: 0.1896773485307811,  L_y: 0.48221794795789386,  L_z: 0.00754718176430757\n",
      "step: 4000, loss: 0.510349398590037, L_x: 0.19453646582418999,  L_y: 0.48362400728055926,  L_z: 0.003635872363529338\n",
      "step: 4250, loss: 0.5048667149007928, L_x: 0.18677755500268234,  L_y: 0.47928138452709457,  L_z: 0.0034537874367149973\n",
      "step: 4500, loss: 0.5025039195919795, L_x: 0.1862443560123042,  L_y: 0.47786315469431984,  L_z: 0.0030081646482146155\n",
      "step: 4750, loss: 0.5006993032463992, L_x: 0.18607521015630904,  L_y: 0.47632395232335245,  L_z: 0.0028839149537079342\n",
      "step: 5000, loss: 0.49793017973677867, L_x: 0.18738953519878532,  L_y: 0.47447824787927195,  L_z: 0.0023564891688140784\n",
      "step: 0, loss: 0.6807507052355513, L_x: 0.49379399241967603,  L_y: 0.6173805220427642,  L_z: 0.006995391975409765\n",
      "step: 250, loss: 0.6392935035847475, L_x: 0.4930429012649068,  L_y: 0.5766416160014858,  L_z: 0.006673798728385515\n",
      "step: 500, loss: 0.6081144708092596, L_x: 0.48769724314434126,  L_y: 0.5489793271189333,  L_z: 0.005182709687946095\n",
      "step: 750, loss: 0.6052380012451388, L_x: 0.4845680424416267,  L_y: 0.5478030825360153,  L_z: 0.0044890572324804605\n",
      "step: 1000, loss: 0.5923130242830612, L_x: 0.42271018262098126,  L_y: 0.5371331634366224,  L_z: 0.006454421292170323\n",
      "step: 1250, loss: 0.5841130377779784, L_x: 0.41250391983941226,  L_y: 0.531332302651881,  L_z: 0.005765171571078105\n",
      "step: 1500, loss: 0.5746839829055366, L_x: 0.3715563579966463,  L_y: 0.518971760605014,  L_z: 0.009278293250428931\n",
      "step: 1750, loss: 0.5601787422335347, L_x: 0.36345648429975874,  L_y: 0.5092728034408659,  L_z: 0.007280145181346481\n",
      "step: 2000, loss: 0.5452406818715376, L_x: 0.31913497303403615,  L_y: 0.5004869429933698,  L_z: 0.0064201207873821\n",
      "step: 2250, loss: 0.5278318316573627, L_x: 0.261533220067705,  L_y: 0.4836057161970697,  L_z: 0.00903639672676125\n",
      "step: 2500, loss: 0.5199848639351188, L_x: 0.246561682508309,  L_y: 0.47940186550943514,  L_z: 0.007963415087426363\n",
      "step: 2750, loss: 0.5175212220532088, L_x: 0.2324886052004826,  L_y: 0.47757064050239756,  L_z: 0.008350860515381513\n",
      "step: 3000, loss: 0.516342450549941, L_x: 0.22591462136921256,  L_y: 0.4757919647942901,  L_z: 0.008979511809364803\n",
      "step: 3250, loss: 0.5159718551945729, L_x: 0.2162749066730848,  L_y: 0.4713564140919726,  L_z: 0.011493975217645922\n",
      "step: 3500, loss: 0.5119103100268277, L_x: 0.2220470913485591,  L_y: 0.47359131714179226,  L_z: 0.00805714187508982\n",
      "step: 3750, loss: 0.5103452414349824, L_x: 0.219285551680103,  L_y: 0.4732626443467697,  L_z: 0.0075770209601011915\n",
      "step: 4000, loss: 0.5129929515357065, L_x: 0.2217683971856508,  L_y: 0.47560216598583077,  L_z: 0.007606972915655314\n",
      "step: 4250, loss: 0.5075059159369705, L_x: 0.21380502109538785,  L_y: 0.4729643351533956,  L_z: 0.006580539337018057\n",
      "step: 4500, loss: 0.50614574897819, L_x: 0.21071917803902307,  L_y: 0.47370333856377156,  L_z: 0.005685246305258063\n",
      "step: 4750, loss: 0.5053326866590853, L_x: 0.20952193175172024,  L_y: 0.4736481843132125,  L_z: 0.005366154585350387\n",
      "step: 5000, loss: 0.5038064301605841, L_x: 0.2097129041951816,  L_y: 0.47340204595160895,  L_z: 0.004716546894728495\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\singh\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\aif360\\metrics\\dataset_metric.py:82: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return metric_fun(privileged=False) / metric_fun(privileged=True)\n",
      "c:\\Users\\singh\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\aif360\\metrics\\classification_metric.py:694: RuntimeWarning: invalid value encountered in divide\n",
      "  return np.mean(np.log((b / np.mean(b))**b) / np.mean(b))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('pre3', 'post2')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\singh\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\aif360\\metrics\\classification_metric.py:278: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  TPR=TP / P, TNR=TN / N, FPR=FP / N, FNR=FN / P,\n",
      "c:\\Users\\singh\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\aif360\\metrics\\classification_metric.py:279: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  GTPR=GTP / P, GTNR=GTN / N, GFPR=GFP / N, GFNR=GFN / P,\n",
      "c:\\Users\\singh\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\aif360\\metrics\\dataset_metric.py:82: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return metric_fun(privileged=False) / metric_fun(privileged=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('pre3', 'post3')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\singh\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\aif360\\metrics\\dataset_metric.py:82: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return metric_fun(privileged=False) / metric_fun(privileged=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('pre3', 'post_none')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\singh\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\aif360\\metrics\\dataset_metric.py:82: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return metric_fun(privileged=False) / metric_fun(privileged=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('pre4', 'post1')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\singh\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\cvxpy\\expressions\\expression.py:612: UserWarning: \n",
      "This use of ``*`` has resulted in matrix multiplication.\n",
      "Using ``*`` for matrix multiplication has been deprecated since CVXPY 1.1.\n",
      "    Use ``*`` for matrix-scalar and vector-scalar multiplication.\n",
      "    Use ``@`` for matrix-matrix and matrix-vector multiplication.\n",
      "    Use ``multiply`` for elementwise multiplication.\n",
      "This code path has been hit 1 times so far.\n",
      "\n",
      "  warnings.warn(msg, UserWarning)\n",
      "c:\\Users\\singh\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\cvxpy\\expressions\\expression.py:612: UserWarning: \n",
      "This use of ``*`` has resulted in matrix multiplication.\n",
      "Using ``*`` for matrix multiplication has been deprecated since CVXPY 1.1.\n",
      "    Use ``*`` for matrix-scalar and vector-scalar multiplication.\n",
      "    Use ``@`` for matrix-matrix and matrix-vector multiplication.\n",
      "    Use ``multiply`` for elementwise multiplication.\n",
      "This code path has been hit 2 times so far.\n",
      "\n",
      "  warnings.warn(msg, UserWarning)\n",
      "c:\\Users\\singh\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\cvxpy\\expressions\\expression.py:612: UserWarning: \n",
      "This use of ``*`` has resulted in matrix multiplication.\n",
      "Using ``*`` for matrix multiplication has been deprecated since CVXPY 1.1.\n",
      "    Use ``*`` for matrix-scalar and vector-scalar multiplication.\n",
      "    Use ``@`` for matrix-matrix and matrix-vector multiplication.\n",
      "    Use ``multiply`` for elementwise multiplication.\n",
      "This code path has been hit 3 times so far.\n",
      "\n",
      "  warnings.warn(msg, UserWarning)\n",
      "c:\\Users\\singh\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\cvxpy\\expressions\\expression.py:612: UserWarning: \n",
      "This use of ``*`` has resulted in matrix multiplication.\n",
      "Using ``*`` for matrix multiplication has been deprecated since CVXPY 1.1.\n",
      "    Use ``*`` for matrix-scalar and vector-scalar multiplication.\n",
      "    Use ``@`` for matrix-matrix and matrix-vector multiplication.\n",
      "    Use ``multiply`` for elementwise multiplication.\n",
      "This code path has been hit 4 times so far.\n",
      "\n",
      "  warnings.warn(msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized Preprocessing: Objective converged to 0.012569\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\singh\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\aif360\\algorithms\\preprocessing\\optim_preproc.py:261: FutureWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
      "  df2.loc[:, dfMap.columns.names] = [draws_possible[x] for x in draw_inds]\n",
      "c:\\Users\\singh\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\cvxpy\\expressions\\expression.py:612: UserWarning: \n",
      "This use of ``*`` has resulted in matrix multiplication.\n",
      "Using ``*`` for matrix multiplication has been deprecated since CVXPY 1.1.\n",
      "    Use ``*`` for matrix-scalar and vector-scalar multiplication.\n",
      "    Use ``@`` for matrix-matrix and matrix-vector multiplication.\n",
      "    Use ``multiply`` for elementwise multiplication.\n",
      "This code path has been hit 5 times so far.\n",
      "\n",
      "  warnings.warn(msg, UserWarning)\n",
      "c:\\Users\\singh\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\cvxpy\\expressions\\expression.py:612: UserWarning: \n",
      "This use of ``*`` has resulted in matrix multiplication.\n",
      "Using ``*`` for matrix multiplication has been deprecated since CVXPY 1.1.\n",
      "    Use ``*`` for matrix-scalar and vector-scalar multiplication.\n",
      "    Use ``@`` for matrix-matrix and matrix-vector multiplication.\n",
      "    Use ``multiply`` for elementwise multiplication.\n",
      "This code path has been hit 6 times so far.\n",
      "\n",
      "  warnings.warn(msg, UserWarning)\n",
      "c:\\Users\\singh\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\cvxpy\\expressions\\expression.py:612: UserWarning: \n",
      "This use of ``*`` has resulted in matrix multiplication.\n",
      "Using ``*`` for matrix multiplication has been deprecated since CVXPY 1.1.\n",
      "    Use ``*`` for matrix-scalar and vector-scalar multiplication.\n",
      "    Use ``@`` for matrix-matrix and matrix-vector multiplication.\n",
      "    Use ``multiply`` for elementwise multiplication.\n",
      "This code path has been hit 7 times so far.\n",
      "\n",
      "  warnings.warn(msg, UserWarning)\n",
      "c:\\Users\\singh\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\cvxpy\\expressions\\expression.py:612: UserWarning: \n",
      "This use of ``*`` has resulted in matrix multiplication.\n",
      "Using ``*`` for matrix multiplication has been deprecated since CVXPY 1.1.\n",
      "    Use ``*`` for matrix-scalar and vector-scalar multiplication.\n",
      "    Use ``@`` for matrix-matrix and matrix-vector multiplication.\n",
      "    Use ``multiply`` for elementwise multiplication.\n",
      "This code path has been hit 8 times so far.\n",
      "\n",
      "  warnings.warn(msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized Preprocessing: Objective converged to 0.010205\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\singh\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\aif360\\algorithms\\preprocessing\\optim_preproc.py:261: FutureWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
      "  df2.loc[:, dfMap.columns.names] = [draws_possible[x] for x in draw_inds]\n",
      "c:\\Users\\singh\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\cvxpy\\expressions\\expression.py:612: UserWarning: \n",
      "This use of ``*`` has resulted in matrix multiplication.\n",
      "Using ``*`` for matrix multiplication has been deprecated since CVXPY 1.1.\n",
      "    Use ``*`` for matrix-scalar and vector-scalar multiplication.\n",
      "    Use ``@`` for matrix-matrix and matrix-vector multiplication.\n",
      "    Use ``multiply`` for elementwise multiplication.\n",
      "This code path has been hit 9 times so far.\n",
      "\n",
      "  warnings.warn(msg, UserWarning)\n",
      "c:\\Users\\singh\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\cvxpy\\expressions\\expression.py:612: UserWarning: \n",
      "This use of ``*`` has resulted in matrix multiplication.\n",
      "Using ``*`` for matrix multiplication has been deprecated since CVXPY 1.1.\n",
      "    Use ``*`` for matrix-scalar and vector-scalar multiplication.\n",
      "    Use ``@`` for matrix-matrix and matrix-vector multiplication.\n",
      "    Use ``multiply`` for elementwise multiplication.\n",
      "This code path has been hit 10 times so far.\n",
      "\n",
      "  warnings.warn(msg, UserWarning)\n",
      "c:\\Users\\singh\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\cvxpy\\expressions\\expression.py:612: UserWarning: \n",
      "This use of ``*`` has resulted in matrix multiplication.\n",
      "Using ``*`` for matrix multiplication has been deprecated since CVXPY 1.1.\n",
      "    Use ``*`` for matrix-scalar and vector-scalar multiplication.\n",
      "    Use ``@`` for matrix-matrix and matrix-vector multiplication.\n",
      "    Use ``multiply`` for elementwise multiplication.\n",
      "This code path has been hit 11 times so far.\n",
      "\n",
      "  warnings.warn(msg, UserWarning)\n",
      "c:\\Users\\singh\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\cvxpy\\expressions\\expression.py:612: UserWarning: \n",
      "This use of ``*`` has resulted in matrix multiplication.\n",
      "Using ``*`` for matrix multiplication has been deprecated since CVXPY 1.1.\n",
      "    Use ``*`` for matrix-scalar and vector-scalar multiplication.\n",
      "    Use ``@`` for matrix-matrix and matrix-vector multiplication.\n",
      "    Use ``multiply`` for elementwise multiplication.\n",
      "This code path has been hit 12 times so far.\n",
      "\n",
      "  warnings.warn(msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized Preprocessing: Objective converged to 0.012263\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\singh\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\aif360\\algorithms\\preprocessing\\optim_preproc.py:261: FutureWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
      "  df2.loc[:, dfMap.columns.names] = [draws_possible[x] for x in draw_inds]\n",
      "c:\\Users\\singh\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\aif360\\algorithms\\postprocessing\\reject_option_classification.py:160: UserWarning: Unable to satisy fairness constraints\n",
      "  warn(\"Unable to satisy fairness constraints\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('pre4', 'post2')\n",
      "('pre4', 'post3')\n",
      "('pre4', 'post_none')\n",
      "('pre_none', 'post1')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\singh\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\aif360\\algorithms\\postprocessing\\reject_option_classification.py:160: UserWarning: Unable to satisy fairness constraints\n",
      "  warn(\"Unable to satisy fairness constraints\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('pre_none', 'post2')\n",
      "('pre_none', 'post3')\n",
      "('pre_none', 'post_none')\n"
     ]
    }
   ],
   "source": [
    "bc,bs = test_preprocessing_combinations(dataset_orig_train, dataset_orig_valid, dataset_orig_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('pre1', 'post3')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.012884583961401685"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
