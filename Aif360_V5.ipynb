{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install aif360[all]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "hyznjya61jOQ",
        "outputId": "e4daf550-bd23-45a3-e63e-ff1c131cd39a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting aif360[all]\n",
            "  Downloading aif360-0.5.0-py3-none-any.whl (214 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/214.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━\u001b[0m \u001b[32m204.8/214.1 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m214.1/214.1 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.16 in /usr/local/lib/python3.10/dist-packages (from aif360[all]) (1.22.4)\n",
            "Requirement already satisfied: scipy>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from aif360[all]) (1.10.1)\n",
            "Requirement already satisfied: pandas>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from aif360[all]) (1.5.3)\n",
            "Requirement already satisfied: scikit-learn>=1.0 in /usr/local/lib/python3.10/dist-packages (from aif360[all]) (1.2.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from aif360[all]) (3.7.1)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.10/dist-packages (from aif360[all]) (0.12.2)\n",
            "Collecting jinja2<3.1.0 (from aif360[all])\n",
            "  Downloading Jinja2-3.0.3-py3-none-any.whl (133 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.6/133.6 kB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ipympl (from aif360[all])\n",
            "  Downloading ipympl-0.9.3-py2.py3-none-any.whl (511 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m511.6/511.6 kB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting adversarial-robustness-toolbox>=1.0.0 (from aif360[all])\n",
            "  Downloading adversarial_robustness_toolbox-1.15.0-py3-none-any.whl (1.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m49.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pytest>=3.5 in /usr/local/lib/python3.10/dist-packages (from aif360[all]) (7.2.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from aif360[all]) (2.0.1+cu118)\n",
            "Collecting BlackBoxAuditing (from aif360[all])\n",
            "  Downloading BlackBoxAuditing-0.1.54.tar.gz (2.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m70.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tensorflow>=1.13.1 in /usr/local/lib/python3.10/dist-packages (from aif360[all]) (2.12.0)\n",
            "Collecting sphinx<2 (from aif360[all])\n",
            "  Downloading Sphinx-1.8.6-py2.py3-none-any.whl (3.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m42.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tempeh (from aif360[all])\n",
            "  Downloading tempeh-0.1.12-py3-none-any.whl (39 kB)\n",
            "Requirement already satisfied: cvxpy>=1.0 in /usr/local/lib/python3.10/dist-packages (from aif360[all]) (1.3.1)\n",
            "Collecting jupyter (from aif360[all])\n",
            "  Downloading jupyter-1.0.0-py2.py3-none-any.whl (2.7 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from aif360[all]) (4.65.0)\n",
            "Collecting igraph[plotting] (from aif360[all])\n",
            "  Downloading igraph-0.10.5-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m44.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting fairlearn~=0.7 (from aif360[all])\n",
            "  Downloading fairlearn-0.8.0-py3-none-any.whl (235 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.0/235.0 kB\u001b[0m \u001b[31m23.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: rpy2 in /usr/local/lib/python3.10/dist-packages (from aif360[all]) (3.5.5)\n",
            "Collecting lime (from aif360[all])\n",
            "  Downloading lime-0.2.0.1.tar.gz (275 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m275.7/275.7 kB\u001b[0m \u001b[31m23.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: lightgbm in /usr/local/lib/python3.10/dist-packages (from aif360[all]) (3.3.5)\n",
            "Collecting sphinx-rtd-theme (from aif360[all])\n",
            "  Downloading sphinx_rtd_theme-1.2.2-py2.py3-none-any.whl (2.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m67.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting scikit-learn>=1.0 (from aif360[all])\n",
            "  Downloading scikit_learn-1.1.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.5/30.5 MB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from adversarial-robustness-toolbox>=1.0.0->aif360[all]) (1.16.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from adversarial-robustness-toolbox>=1.0.0->aif360[all]) (67.7.2)\n",
            "Requirement already satisfied: osqp>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from cvxpy>=1.0->aif360[all]) (0.6.2.post8)\n",
            "Requirement already satisfied: ecos>=2 in /usr/local/lib/python3.10/dist-packages (from cvxpy>=1.0->aif360[all]) (2.0.12)\n",
            "Requirement already satisfied: scs>=1.1.6 in /usr/local/lib/python3.10/dist-packages (from cvxpy>=1.0->aif360[all]) (3.2.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2<3.1.0->aif360[all]) (2.1.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24.0->aif360[all]) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24.0->aif360[all]) (2022.7.1)\n",
            "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.10/dist-packages (from pytest>=3.5->aif360[all]) (23.1.0)\n",
            "Requirement already satisfied: iniconfig in /usr/local/lib/python3.10/dist-packages (from pytest>=3.5->aif360[all]) (2.0.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from pytest>=3.5->aif360[all]) (23.1)\n",
            "Requirement already satisfied: pluggy<2.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from pytest>=3.5->aif360[all]) (1.2.0)\n",
            "Requirement already satisfied: exceptiongroup>=1.0.0rc8 in /usr/local/lib/python3.10/dist-packages (from pytest>=3.5->aif360[all]) (1.1.1)\n",
            "Requirement already satisfied: tomli>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from pytest>=3.5->aif360[all]) (2.0.1)\n",
            "Requirement already satisfied: joblib>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0->aif360[all]) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0->aif360[all]) (3.1.0)\n",
            "Requirement already satisfied: Pygments>=2.0 in /usr/local/lib/python3.10/dist-packages (from sphinx<2->aif360[all]) (2.14.0)\n",
            "Requirement already satisfied: docutils<0.18,>=0.11 in /usr/local/lib/python3.10/dist-packages (from sphinx<2->aif360[all]) (0.16)\n",
            "Requirement already satisfied: snowballstemmer>=1.1 in /usr/local/lib/python3.10/dist-packages (from sphinx<2->aif360[all]) (2.2.0)\n",
            "Requirement already satisfied: babel!=2.0,>=1.3 in /usr/local/lib/python3.10/dist-packages (from sphinx<2->aif360[all]) (2.12.1)\n",
            "Requirement already satisfied: alabaster<0.8,>=0.7 in /usr/local/lib/python3.10/dist-packages (from sphinx<2->aif360[all]) (0.7.13)\n",
            "Requirement already satisfied: imagesize in /usr/local/lib/python3.10/dist-packages (from sphinx<2->aif360[all]) (1.4.1)\n",
            "Requirement already satisfied: requests>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from sphinx<2->aif360[all]) (2.27.1)\n",
            "Collecting sphinxcontrib-websupport (from sphinx<2->aif360[all])\n",
            "  Downloading sphinxcontrib_websupport-1.2.4-py2.py3-none-any.whl (39 kB)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.13.1->aif360[all]) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.13.1->aif360[all]) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.13.1->aif360[all]) (23.5.26)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.13.1->aif360[all]) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.13.1->aif360[all]) (0.2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.13.1->aif360[all]) (1.56.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.13.1->aif360[all]) (3.8.0)\n",
            "Requirement already satisfied: jax>=0.3.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.13.1->aif360[all]) (0.4.10)\n",
            "Requirement already satisfied: keras<2.13,>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.13.1->aif360[all]) (2.12.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.13.1->aif360[all]) (16.0.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.13.1->aif360[all]) (3.3.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.13.1->aif360[all]) (3.20.3)\n",
            "Requirement already satisfied: tensorboard<2.13,>=2.12 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.13.1->aif360[all]) (2.12.3)\n",
            "Requirement already satisfied: tensorflow-estimator<2.13,>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.13.1->aif360[all]) (2.12.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.13.1->aif360[all]) (2.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.13.1->aif360[all]) (4.6.3)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.13.1->aif360[all]) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.13.1->aif360[all]) (0.32.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from BlackBoxAuditing->aif360[all]) (3.1)\n",
            "Collecting texttable>=1.6.2 (from igraph[plotting]->aif360[all])\n",
            "  Downloading texttable-1.6.7-py2.py3-none-any.whl (10 kB)\n",
            "Collecting cairocffi>=1.2.0 (from igraph[plotting]->aif360[all])\n",
            "  Downloading cairocffi-1.6.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.3/75.3 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: ipython<9 in /usr/local/lib/python3.10/dist-packages (from ipympl->aif360[all]) (7.34.0)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.10/dist-packages (from ipympl->aif360[all]) (0.2.0)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from ipympl->aif360[all]) (8.4.0)\n",
            "Requirement already satisfied: traitlets<6 in /usr/local/lib/python3.10/dist-packages (from ipympl->aif360[all]) (5.7.1)\n",
            "Requirement already satisfied: ipywidgets<9,>=7.6.0 in /usr/local/lib/python3.10/dist-packages (from ipympl->aif360[all]) (7.7.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->aif360[all]) (1.1.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->aif360[all]) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->aif360[all]) (4.40.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->aif360[all]) (1.4.4)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->aif360[all]) (3.1.0)\n",
            "Requirement already satisfied: notebook in /usr/local/lib/python3.10/dist-packages (from jupyter->aif360[all]) (6.4.8)\n",
            "Collecting qtconsole (from jupyter->aif360[all])\n",
            "  Downloading qtconsole-5.4.3-py3-none-any.whl (121 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.9/121.9 kB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: jupyter-console in /usr/local/lib/python3.10/dist-packages (from jupyter->aif360[all]) (6.1.0)\n",
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.10/dist-packages (from jupyter->aif360[all]) (6.5.4)\n",
            "Requirement already satisfied: ipykernel in /usr/local/lib/python3.10/dist-packages (from jupyter->aif360[all]) (5.5.6)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (from lightgbm->aif360[all]) (0.40.0)\n",
            "Requirement already satisfied: scikit-image>=0.12 in /usr/local/lib/python3.10/dist-packages (from lime->aif360[all]) (0.19.3)\n",
            "Requirement already satisfied: cffi>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from rpy2->aif360[all]) (1.15.1)\n",
            "Requirement already satisfied: tzlocal in /usr/local/lib/python3.10/dist-packages (from rpy2->aif360[all]) (5.0.1)\n",
            "Collecting sphinxcontrib-jquery<5,>=4 (from sphinx-rtd-theme->aif360[all])\n",
            "  Downloading sphinxcontrib_jquery-4.1-py2.py3-none-any.whl (121 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.1/121.1 kB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting memory-profiler (from tempeh->aif360[all])\n",
            "  Downloading memory_profiler-0.61.0-py3-none-any.whl (31 kB)\n",
            "Collecting shap (from tempeh->aif360[all])\n",
            "  Downloading shap-0.41.0-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (572 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m572.6/572.6 kB\u001b[0m \u001b[31m53.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->aif360[all]) (3.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->aif360[all]) (1.11.1)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch->aif360[all]) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->aif360[all]) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->aif360[all]) (16.0.6)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.10.0->rpy2->aif360[all]) (2.21)\n",
            "Collecting jedi>=0.16 (from ipython<9->ipympl->aif360[all])\n",
            "  Downloading jedi-0.18.2-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m90.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython<9->ipympl->aif360[all]) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython<9->ipympl->aif360[all]) (0.7.5)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ipython<9->ipympl->aif360[all]) (3.0.38)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython<9->ipympl->aif360[all]) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython<9->ipympl->aif360[all]) (0.1.6)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython<9->ipympl->aif360[all]) (4.8.0)\n",
            "Requirement already satisfied: widgetsnbextension~=3.6.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets<9,>=7.6.0->ipympl->aif360[all]) (3.6.4)\n",
            "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets<9,>=7.6.0->ipympl->aif360[all]) (3.0.7)\n",
            "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.10/dist-packages (from ipykernel->jupyter->aif360[all]) (6.1.12)\n",
            "Requirement already satisfied: tornado>=4.2 in /usr/local/lib/python3.10/dist-packages (from ipykernel->jupyter->aif360[all]) (6.3.1)\n",
            "Requirement already satisfied: ml-dtypes>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from jax>=0.3.15->tensorflow>=1.13.1->aif360[all]) (0.2.0)\n",
            "Requirement already satisfied: qdldl in /usr/local/lib/python3.10/dist-packages (from osqp>=0.4.1->cvxpy>=1.0->aif360[all]) (0.1.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.0.0->sphinx<2->aif360[all]) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.0.0->sphinx<2->aif360[all]) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests>=2.0.0->sphinx<2->aif360[all]) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.0.0->sphinx<2->aif360[all]) (3.4)\n",
            "Requirement already satisfied: imageio>=2.4.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.12->lime->aif360[all]) (2.25.1)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.12->lime->aif360[all]) (2023.4.12)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.12->lime->aif360[all]) (1.4.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow>=1.13.1->aif360[all]) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow>=1.13.1->aif360[all]) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow>=1.13.1->aif360[all]) (3.4.3)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow>=1.13.1->aif360[all]) (0.7.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow>=1.13.1->aif360[all]) (2.3.6)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from memory-profiler->tempeh->aif360[all]) (5.9.5)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter->aif360[all]) (4.9.2)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter->aif360[all]) (4.11.2)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter->aif360[all]) (6.0.0)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter->aif360[all]) (0.7.1)\n",
            "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter->aif360[all]) (0.4)\n",
            "Requirement already satisfied: jupyter-core>=4.7 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter->aif360[all]) (5.3.1)\n",
            "Requirement already satisfied: jupyterlab-pygments in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter->aif360[all]) (0.2.2)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter->aif360[all]) (0.8.4)\n",
            "Requirement already satisfied: nbclient>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter->aif360[all]) (0.8.0)\n",
            "Requirement already satisfied: nbformat>=5.1 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter->aif360[all]) (5.9.0)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter->aif360[all]) (1.5.0)\n",
            "Requirement already satisfied: tinycss2 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter->aif360[all]) (1.2.1)\n",
            "Requirement already satisfied: pyzmq>=17 in /usr/local/lib/python3.10/dist-packages (from notebook->jupyter->aif360[all]) (23.2.1)\n",
            "Requirement already satisfied: argon2-cffi in /usr/local/lib/python3.10/dist-packages (from notebook->jupyter->aif360[all]) (21.3.0)\n",
            "Requirement already satisfied: nest-asyncio>=1.5 in /usr/local/lib/python3.10/dist-packages (from notebook->jupyter->aif360[all]) (1.5.6)\n",
            "Requirement already satisfied: Send2Trash>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from notebook->jupyter->aif360[all]) (1.8.2)\n",
            "Requirement already satisfied: terminado>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from notebook->jupyter->aif360[all]) (0.17.1)\n",
            "Requirement already satisfied: prometheus-client in /usr/local/lib/python3.10/dist-packages (from notebook->jupyter->aif360[all]) (0.17.0)\n",
            "Collecting qtpy>=2.0.1 (from qtconsole->jupyter->aif360[all])\n",
            "  Downloading QtPy-2.3.1-py3-none-any.whl (84 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.9/84.9 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting slicer==0.0.7 (from shap->tempeh->aif360[all])\n",
            "  Downloading slicer-0.0.7-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (from shap->tempeh->aif360[all]) (0.56.4)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from shap->tempeh->aif360[all]) (2.2.1)\n",
            "Requirement already satisfied: sphinxcontrib-serializinghtml in /usr/local/lib/python3.10/dist-packages (from sphinxcontrib-websupport->sphinx<2->aif360[all]) (1.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->aif360[all]) (1.3.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow>=1.13.1->aif360[all]) (5.3.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow>=1.13.1->aif360[all]) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow>=1.13.1->aif360[all]) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow>=1.13.1->aif360[all]) (1.3.1)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython<9->ipympl->aif360[all]) (0.8.3)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.10/dist-packages (from jupyter-core>=4.7->nbconvert->jupyter->aif360[all]) (3.7.0)\n",
            "Requirement already satisfied: fastjsonschema in /usr/local/lib/python3.10/dist-packages (from nbformat>=5.1->nbconvert->jupyter->aif360[all]) (2.17.1)\n",
            "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.10/dist-packages (from nbformat>=5.1->nbconvert->jupyter->aif360[all]) (4.3.3)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->ipython<9->ipympl->aif360[all]) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython<9->ipympl->aif360[all]) (0.2.6)\n",
            "Requirement already satisfied: argon2-cffi-bindings in /usr/local/lib/python3.10/dist-packages (from argon2-cffi->notebook->jupyter->aif360[all]) (21.2.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->nbconvert->jupyter->aif360[all]) (2.4.1)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->nbconvert->jupyter->aif360[all]) (0.5.1)\n",
            "Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba->shap->tempeh->aif360[all]) (0.39.1)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat>=5.1->nbconvert->jupyter->aif360[all]) (0.19.3)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow>=1.13.1->aif360[all]) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow>=1.13.1->aif360[all]) (3.2.2)\n",
            "Building wheels for collected packages: BlackBoxAuditing, lime\n",
            "  Building wheel for BlackBoxAuditing (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for BlackBoxAuditing: filename=BlackBoxAuditing-0.1.54-py2.py3-none-any.whl size=1394754 sha256=2bc2b3265898cd2132e52634afb1ff25fb43b236b476fc8d689b983f8072b7f6\n",
            "  Stored in directory: /root/.cache/pip/wheels/c0/4f/b1/80e1b0790df07536470758fe0a4f9ff8fa942fd9fe30bbb192\n",
            "  Building wheel for lime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for lime: filename=lime-0.2.0.1-py3-none-any.whl size=283839 sha256=1db0e32da0836a78422020c0c29f0efc6cd98bd98a9c96e8b7f3b3e8ac4ce0ff\n",
            "  Stored in directory: /root/.cache/pip/wheels/fd/a2/af/9ac0a1a85a27f314a06b39e1f492bee1547d52549a4606ed89\n",
            "Successfully built BlackBoxAuditing lime\n",
            "Installing collected packages: texttable, sphinxcontrib-websupport, slicer, qtpy, memory-profiler, jinja2, jedi, igraph, sphinx, scikit-learn, cairocffi, sphinxcontrib-jquery, shap, lime, fairlearn, BlackBoxAuditing, aif360, adversarial-robustness-toolbox, tempeh, sphinx-rtd-theme, qtconsole, jupyter, ipympl\n",
            "  Attempting uninstall: jinja2\n",
            "    Found existing installation: Jinja2 3.1.2\n",
            "    Uninstalling Jinja2-3.1.2:\n",
            "      Successfully uninstalled Jinja2-3.1.2\n",
            "  Attempting uninstall: sphinx\n",
            "    Found existing installation: Sphinx 3.5.4\n",
            "    Uninstalling Sphinx-3.5.4:\n",
            "      Successfully uninstalled Sphinx-3.5.4\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 1.2.2\n",
            "    Uninstalling scikit-learn-1.2.2:\n",
            "      Successfully uninstalled scikit-learn-1.2.2\n",
            "Successfully installed BlackBoxAuditing-0.1.54 adversarial-robustness-toolbox-1.15.0 aif360-0.5.0 cairocffi-1.6.0 fairlearn-0.8.0 igraph-0.10.5 ipympl-0.9.3 jedi-0.18.2 jinja2-3.0.3 jupyter-1.0.0 lime-0.2.0.1 memory-profiler-0.61.0 qtconsole-5.4.3 qtpy-2.3.1 scikit-learn-1.1.3 shap-0.41.0 slicer-0.0.7 sphinx-1.8.6 sphinx-rtd-theme-1.2.2 sphinxcontrib-jquery-4.1 sphinxcontrib-websupport-1.2.4 tempeh-0.1.12 texttable-1.6.7\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "sphinxcontrib"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cd /usr/local/lib/python3.10/dist-packages/aif360/data/raw/adult\n",
        "!wget https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data -P /usr/local/lib/python3.10/dist-packages/aif360/data/raw/adult\n",
        "!wget https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.names -P /usr/local/lib/python3.10/dist-packages/aif360/data/raw/adult\n",
        "!wget https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.test -P /usr/local/lib/python3.10/dist-packages/aif360/data/raw/adult\n",
        "\n",
        "# !wget https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data\n",
        "# !wget https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.names\n",
        "# !wget https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.test"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DCzWHKd81pgm",
        "outputId": "0b363224-9c88-472d-b5eb-e3ef658f9074"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-07-03 09:05:09--  https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data\n",
            "Resolving archive.ics.uci.edu (archive.ics.uci.edu)... 128.195.10.252\n",
            "Connecting to archive.ics.uci.edu (archive.ics.uci.edu)|128.195.10.252|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified\n",
            "Saving to: ‘/usr/local/lib/python3.10/dist-packages/aif360/data/raw/adult/adult.data’\n",
            "\n",
            "adult.data              [    <=>             ]   3.79M  3.39MB/s    in 1.1s    \n",
            "\n",
            "2023-07-03 09:05:11 (3.39 MB/s) - ‘/usr/local/lib/python3.10/dist-packages/aif360/data/raw/adult/adult.data’ saved [3974305]\n",
            "\n",
            "--2023-07-03 09:05:11--  https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.names\n",
            "Resolving archive.ics.uci.edu (archive.ics.uci.edu)... 128.195.10.252\n",
            "Connecting to archive.ics.uci.edu (archive.ics.uci.edu)|128.195.10.252|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified\n",
            "Saving to: ‘/usr/local/lib/python3.10/dist-packages/aif360/data/raw/adult/adult.names’\n",
            "\n",
            "adult.names             [ <=>                ]   5.11K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-07-03 09:05:12 (82.7 MB/s) - ‘/usr/local/lib/python3.10/dist-packages/aif360/data/raw/adult/adult.names’ saved [5229]\n",
            "\n",
            "--2023-07-03 09:05:12--  https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.test\n",
            "Resolving archive.ics.uci.edu (archive.ics.uci.edu)... 128.195.10.252\n",
            "Connecting to archive.ics.uci.edu (archive.ics.uci.edu)|128.195.10.252|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified\n",
            "Saving to: ‘/usr/local/lib/python3.10/dist-packages/aif360/data/raw/adult/adult.test’\n",
            "\n",
            "adult.test              [    <=>             ]   1.91M  1.99MB/s    in 1.0s    \n",
            "\n",
            "2023-07-03 09:05:13 (1.99 MB/s) - ‘/usr/local/lib/python3.10/dist-packages/aif360/data/raw/adult/adult.test’ saved [2003153]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wrpIlvuy1idw"
      },
      "outputs": [],
      "source": [
        "from aif360.metrics import BinaryLabelDatasetMetric\n",
        "from aif360.metrics import ClassificationMetric\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "\n",
        "from aif360.algorithms.preprocessing.optim_preproc_helpers.data_preproc_functions \\\n",
        "import load_preproc_data_adult #, load_preproc_data_german, load_preproc_data_compas\n",
        "\n",
        "from aif360.algorithms.preprocessing.optim_preproc_helpers.distortion_functions\\\n",
        "            import get_distortion_adult #, get_distortion_german, get_distortion_compas\n",
        "\n",
        "# Bias mitigation preproc-techniques\n",
        "\n",
        "from aif360.sklearn.preprocessing import Reweighing\n",
        "\n",
        "from aif360.algorithms.preprocessing import Reweighing\n",
        "from aif360.algorithms.preprocessing.optim_preproc import OptimPreproc\n",
        "from aif360.algorithms.preprocessing.optim_preproc_helpers.opt_tools import OptTools\n",
        "from aif360.algorithms.preprocessing import DisparateImpactRemover\n",
        "from aif360.algorithms.preprocessing.lfr import LFR\n",
        "\n",
        "from aif360.algorithms.preprocessing.optim_preproc_helpers.data_preproc_functions\\\n",
        "import load_preproc_data_adult\n",
        "\n",
        "# from aif360.algorithms.preprocessing.optim_preproc_helpers import data_preproc_functions\n",
        "\n",
        "from aif360.algorithms.preprocessing.optim_preproc_helpers.opt_tools import OptTools\n",
        "\n",
        "#from aif360.algorithms.inprocessing import PrejudiceRemover\n",
        "\n",
        "# from aif360.datasets.lime_encoder import LimeEncoder\n",
        "# import lime\n",
        "# from lime.lime_tabular import LimeTabularExplainer\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from aif360.datasets import BinaryLabelDataset\n",
        "from aif360.datasets import StructuredDataset\n",
        "from aif360.datasets import AdultDataset\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "\n",
        "# inprocessing\n",
        "# from aif360.algorithms.inprocessing.adversarial_debiasing import AdversarialDebiasing\n",
        "# from aif360.sklearn.inprocessing import AdversarialDebiasing\n",
        "import tensorflow as tf\n",
        "from aif360.algorithms.inprocessing import PrejudiceRemover, GerryFairClassifier, ExponentiatedGradientReduction, AdversarialDebiasing\n",
        "\n",
        "\n",
        "# postprocessing\n",
        "from aif360.algorithms.postprocessing.reject_option_classification\\\n",
        "        import RejectOptionClassification\n",
        "# Odds equalizing post-processing algorithm\n",
        "from aif360.algorithms.postprocessing.calibrated_eq_odds_postprocessing import CalibratedEqOddsPostprocessing\n",
        "# from aif360.algorithms.postprocessing.eq_odds_postprocessing import EqOddsPostprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5sOSH-7z1id1",
        "outputId": "e0b148b4-8fbb-4b0b-8136-d49a5d7356ac"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['race',\n",
              " 'sex',\n",
              " 'Age (decade)=10',\n",
              " 'Age (decade)=20',\n",
              " 'Age (decade)=30',\n",
              " 'Age (decade)=40',\n",
              " 'Age (decade)=50',\n",
              " 'Age (decade)=60',\n",
              " 'Age (decade)=>=70',\n",
              " 'Education Years=6',\n",
              " 'Education Years=7',\n",
              " 'Education Years=8',\n",
              " 'Education Years=9',\n",
              " 'Education Years=10',\n",
              " 'Education Years=11',\n",
              " 'Education Years=12',\n",
              " 'Education Years=<6',\n",
              " 'Education Years=>12']"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "privileged_groups = [{'sex': 1}]\n",
        "unprivileged_groups = [{'sex': 0}]\n",
        "dataset_orig = load_preproc_data_adult(['sex'])\n",
        "dataset_orig.feature_names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E3HNFAcy1id2"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler= StandardScaler()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3MwnFghA1id2"
      },
      "outputs": [],
      "source": [
        "dataset_orig_train, dataset_orig_vt = dataset_orig.split([0.7], shuffle=False)\n",
        "dataset_orig_valid, dataset_orig_test = dataset_orig_vt.split([0.5], shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kIPzbDz41id3"
      },
      "outputs": [],
      "source": [
        "protected = 'sex'\n",
        "# dataset_orig_train.features = scaler.fit_transform(dataset_orig_train.features)\n",
        "# dataset_orig_valid.features = scaler.fit_transform(dataset_orig_valid.features)\n",
        "# dataset_orig_test.features = scaler.fit_transform(dataset_orig_test.features)\n",
        "\n",
        "index = dataset_orig_train.feature_names.index(protected)\n",
        "\n",
        "# di = DisparateImpactRemover(repair_level=1)\n",
        "# data = di.fit_transform(dataset_orig_train)\n",
        "# data\n",
        "\n",
        "lmod = LogisticRegression()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zTeSYECP1id3"
      },
      "outputs": [],
      "source": [
        "debug = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dIjilz_D1id3"
      },
      "outputs": [],
      "source": [
        "def pred_preproc_fit_model(train, valid, test):\n",
        "\n",
        "    # fitting on training data\n",
        "    scale_orig = StandardScaler()\n",
        "    x_train = scale_orig.fit_transform(train.features)\n",
        "    y_train = train.labels.ravel()\n",
        "    # w_train = train.instance_weights.ravel()\n",
        "\n",
        "    if debug==1:\n",
        "        print(\"Scaler OK\")\n",
        "\n",
        "\n",
        "    lmod.fit(x_train, y_train,\n",
        "         sample_weight=train.instance_weights)\n",
        "\n",
        "    if debug==1:\n",
        "        print(\"Model fit OK\")\n",
        "\n",
        "\n",
        "    # prediction on training data\n",
        "    y_train_pred = lmod.predict(x_train)\n",
        "\n",
        "    if debug==1:\n",
        "        print(\"y pred OK\")\n",
        "\n",
        "    # positive class index\n",
        "    pos_ind = np.where(lmod.classes_ == train.favorable_label)[0][0]\n",
        "\n",
        "    if debug==1:\n",
        "        print(\"pos_ind OK\")\n",
        "\n",
        "    train_pred = train.copy(deepcopy=True)\n",
        "    # train_pred_labels = y_train_pred\n",
        "\n",
        "\n",
        "    # prediction on validation and test data and score calculation\n",
        "    valid_pred = valid.copy(deepcopy=True)\n",
        "    X_valid = scale_orig.transform(valid_pred.features)\n",
        "    # y_valid = valid_pred.labels\n",
        "    valid_pred.scores = lmod.predict_proba(X_valid)[:,pos_ind].reshape(-1,1)\n",
        "\n",
        "    if debug==1:\n",
        "        print(\"valid OK\")\n",
        "\n",
        "    test_pred = test.copy(deepcopy=True)\n",
        "    X_test = scale_orig.transform(test_pred.features)\n",
        "    # y_test = test_pred.labels\n",
        "    test_pred.scores = lmod.predict_proba(X_test)[:,pos_ind].reshape(-1,1)\n",
        "\n",
        "    if debug==1:\n",
        "        print(\"test OK\")\n",
        "\n",
        "    num_thresh = 100\n",
        "    ba_arr = np.zeros(num_thresh)\n",
        "    class_thresh_arr = np.linspace(0.01, 0.99, num_thresh)\n",
        "\n",
        "    if debug==1:\n",
        "        print(\"def thresh OK\")\n",
        "\n",
        "\n",
        "\n",
        "    # balanced accuracy, threshold\n",
        "    for idx, class_thresh in enumerate(class_thresh_arr):\n",
        "\n",
        "        fav_inds = valid_pred.scores > class_thresh\n",
        "        valid_pred.labels[fav_inds] = valid_pred.favorable_label\n",
        "        valid_pred.labels[~fav_inds] = valid_pred.unfavorable_label\n",
        "\n",
        "        if debug==1:\n",
        "            print(\"best thresh sel OK\")\n",
        "\n",
        "        classified_metric_valid = ClassificationMetric(valid,\n",
        "                                                 valid_pred,\n",
        "                                                 unprivileged_groups=unprivileged_groups,\n",
        "                                                 privileged_groups=privileged_groups)\n",
        "        if debug==1:\n",
        "            print(\"class metric valid OK\")\n",
        "\n",
        "\n",
        "        ba_arr[idx] = 0.5*(classified_metric_valid.true_positive_rate()\\\n",
        "                       +classified_metric_valid.true_negative_rate())\n",
        "\n",
        "    best_ind = np.where(ba_arr == np.max(ba_arr))[0][0]\n",
        "    best_class_thresh = class_thresh_arr[best_ind]\n",
        "\n",
        "    if debug==1:\n",
        "        print(\"best class thresh OK\")\n",
        "\n",
        "\n",
        "\n",
        "    return train_pred, valid_pred, test_pred, best_class_thresh"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FcAdc4rR1id4"
      },
      "outputs": [],
      "source": [
        "def reweighing(datas, unprivileged_groups=unprivileged_groups,privileged_groups=privileged_groups):\n",
        "\n",
        "#     scale_orig = StandardScaler()\n",
        "#     data= scale_orig.fit_transform(data)\n",
        "    dataset_transf = []\n",
        "    for data in datas:\n",
        "      RW = Reweighing(unprivileged_groups=unprivileged_groups,\n",
        "                    privileged_groups=privileged_groups)\n",
        "\n",
        "      RW.fit(data)\n",
        "\n",
        "      dataset_transf.append(RW.transform(data))\n",
        "\n",
        "    return dataset_transf\n",
        "\n",
        "\n",
        "def disparate_impact_remover(datas, level=1):\n",
        "\n",
        "    # scale_orig = StandardScaler()\n",
        "    # data.features= scale_orig.fit_transform(data.features)\n",
        "\n",
        "    dataset_transf = []\n",
        "    for data in datas:\n",
        "      di = DisparateImpactRemover(repair_level=level)\n",
        "      dataset_transf.append( di.fit_transform(data))\n",
        "\n",
        "    return dataset_transf\n",
        "\n",
        "\n",
        "def lfr(datas, unprivileged_groups=unprivileged_groups,privileged_groups=privileged_groups):\n",
        "\n",
        "    # scale_orig = StandardScaler()\n",
        "    # data.features = scale_orig.fit_transform(data.features)\n",
        "\n",
        "    dataset_transf = []\n",
        "    for data in datas:\n",
        "      TR = LFR(unprivileged_groups=unprivileged_groups,\n",
        "          privileged_groups=privileged_groups,\n",
        "          k=10, Ax=0.1, Ay=1.0, Az=2.0,\n",
        "          verbose=1\n",
        "          )\n",
        "      TR = TR.fit(data, maxiter=5000, maxfun=5000)\n",
        "      dataset_transf.append(TR.transform(data))\n",
        "\n",
        "    return dataset_transf\n",
        "\n",
        "\n",
        "\n",
        "def optim_preproc(datas):\n",
        "\n",
        "  dataset_transf = []\n",
        "  for data in datas:\n",
        "    optim_options = {\n",
        "      \"distortion_fun\": get_distortion_adult,\n",
        "      \"epsilon\": 0.05,\n",
        "      \"clist\": [0.99, 1.99, 2.99],\n",
        "      \"dlist\": [.1, 0.05, 0]\n",
        "    }\n",
        "\n",
        "    OP = OptimPreproc(OptTools, optim_options)\n",
        "    OP = OP.fit(data)\n",
        "    temp = OP.transform(data, transform_Y=True)\n",
        "    dataset_transf.append(data.align_datasets(temp))\n",
        "\n",
        "  return dataset_transf\n",
        "\n",
        "\n",
        "# # From example\n",
        "# def adversial_debiasing(datas):\n",
        "#   dataset_transf = []\n",
        "#   tf.compat.v1.disable_eager_execution()\n",
        "#   tf.logging.set_verbosity(tf.logging.ERROR)\n",
        "\n",
        "#   for data in datas:\n",
        "#     if np.any(data.features==None):\n",
        "#       continue\n",
        "#     # sess = tf.Session()\n",
        "#     with tf.compat.v1.variable_scope('debiased_classifier', reuse=tf.compat.v1.AUTO_REUSE):\n",
        "\n",
        "#       with tf.compat.v1.Session() as sess:\n",
        "#         debiased_model = AdversarialDebiasing(privileged_groups = privileged_groups,\n",
        "#                               unprivileged_groups = unprivileged_groups,\n",
        "#                               scope_name='debiased_classifier',\n",
        "#                               debias=True,\n",
        "#                               sess=sess)\n",
        "#         sess.run(tf.compat.v1.global_variables_initializer())  # Initialize global variables\n",
        "#         sess.run(tf.compat.v1.local_variables_initializer())\n",
        "#         debiased_model.fit(data)\n",
        "\n",
        "\n",
        "#         dataset_transf.append(debiased_model.predict(data))\n",
        "\n",
        "#     sess.close()\n",
        "\n",
        "#   return dataset_transf\n",
        "\n",
        "\n",
        "\n",
        "# def adversial_debiasing(datas):\n",
        "#   dataset_transf = []\n",
        "\n",
        "#   for data in datas:\n",
        "#     if np.any(data.features==None):\n",
        "#       continue\n",
        "#     # sess = tf.Session()\n",
        "#     # with tf.compat.v1.variable_scope('debiased_classifier', reuse=tf.compat.v1.AUTO_REUSE):\n",
        "#     tf.compat.v1.disable_eager_execution()\n",
        "#       # with tf.compat.v1.Session() as sess:\n",
        "#     debiased_model = AdversarialDebiasing(\n",
        "#                                           prot_attr=[data.protected_attributes],\n",
        "#                                           # scope_name='debiased_classifier',\n",
        "#                                           # adversary_loss_weight=0.1,\n",
        "#                                           # num_epochs=50, batch_size=128,\n",
        "#                                           # classifier_num_hidden_units=200,\n",
        "#                                           debias=True, verbose=False, random_state=None)\n",
        "#         # sess.run(tf.compat.v1.global_variables_initializer())  # Initialize global variables\n",
        "#         # sess.run(tf.compat.v1.local_variables_initializer())\n",
        "\n",
        "\n",
        "#     # print(type(data))\n",
        "#     # df = StructuredDataset.convert_to_dataframe(data)[0]\n",
        "#     # # print(df)\n",
        "\n",
        "#     # x = df.drop(data.label_names, axis=1)\n",
        "#     # y = df[data.label_names[0]]\n",
        "\n",
        "#     # print(y)\n",
        "\n",
        "#     # data = StructuredDataset(df, data.label_names, data.protected_attributes, data.instance_weights)\n",
        "\n",
        "#     debiased_model.fit(data.features, data.labels.ravel())\n",
        "\n",
        "\n",
        "#     dataset_transf.append(debiased_model.predict(data))\n",
        "\n",
        "#     # sess.close()\n",
        "\n",
        "#   return dataset_transf\n",
        "\n",
        "# in processing\n",
        "\n",
        "# def adversarial_debiasing_in_proc(test_data):\n",
        "\n",
        "#     #compatibility settings\n",
        "#     sess = tf.compat.v1.Session()\n",
        "#     tf.compat.v1.disable_eager_execution()\n",
        "#     with tf.compat.v1.variable_scope('debiased_classifier', reuse=tf.compat.v1.AUTO_REUSE):\n",
        "#       debiased_model = AdversarialDebiasing(privileged_groups = privileged_groups,\n",
        "#                             unprivileged_groups = unprivileged_groups,\n",
        "#                             scope_name='debiased_classifier',\n",
        "#                             debias=True,\n",
        "#                             sess=sess)\n",
        "#       debiased_model.fit(test_data)\n",
        "#       dataset_debiasing_test = debiased_model.predict(test_data)\n",
        "#     sess.close()\n",
        "#     return dataset_debiasing_test\n",
        "\n",
        "def exp_grad_red_in_proc(test_data):\n",
        "    estimator = LogisticRegression(solver='lbfgs', max_iter=1000)\n",
        "\n",
        "    np.random.seed(42) #need for reproducibility\n",
        "    exp_grad_red = ExponentiatedGradientReduction(estimator=estimator,\n",
        "                                                  constraints=\"EqualizedOdds\",\n",
        "                                                  drop_prot_attr=False)\n",
        "    exp_grad_red.fit(test_data)\n",
        "    exp_grad_red_pred = exp_grad_red.predict(test_data)\n",
        "\n",
        "    return exp_grad_red_pred\n",
        "\n",
        "def gerry_in_proc(test_data):\n",
        "\n",
        "    max_iterations = 50\n",
        "    C = 10\n",
        "    print_flag = True\n",
        "    gamma = .005\n",
        "\n",
        "    fair_model = GerryFairClassifier(C=C, printflag=print_flag, gamma=gamma, fairness_def='FP',\n",
        "                 max_iters=max_iterations, heatmapflag=False)\n",
        "\n",
        "    fair_model.fit(test_data, early_termination=True)\n",
        "\n",
        "    data_pred=fair_model.predict(test_data, threshold=False)\n",
        "\n",
        "    # fav_idx = np.where(lmod.classes_ == test_data.favorable_label)[0][0]\n",
        "    fav_idx = np.where(1 == test_data.favorable_label)[0][0]\n",
        "\n",
        "    y_train_pred = np.zeros_like(data_pred.labels)\n",
        "\n",
        "    y_train_pred[data_pred.labels>=0.5] = data_pred.favorable_label\n",
        "    y_train_pred[~(data_pred.labels>=0.5)] = data_pred.unfavorable_label\n",
        "    data_pred.labels = y_train_pred\n",
        "\n",
        "    return data_pred\n",
        "\n",
        "def prejudice_remover(test_data):\n",
        "    prej=PrejudiceRemover(eta=1.0,sensitive_attr=protected)\n",
        "    prej.fit(test_data)\n",
        "    pred_df=prej.predict(test_data)\n",
        "\n",
        "    return pred_df\n",
        "\n",
        "\n",
        "\n",
        "# post processing\n",
        "\n",
        "def RejOpt(data, data_pred):\n",
        "  # dataset_transf = []\n",
        "  # for data, data_pred in zip(datas, datapreds):\n",
        "    ROC = RejectOptionClassification(unprivileged_groups=unprivileged_groups,\n",
        "                                  privileged_groups=privileged_groups,\n",
        "                                  low_class_thresh=0.01, high_class_thresh=0.99,\n",
        "                                    num_class_thresh=100, num_ROC_margin=50,\n",
        "                                    metric_name=\"Statistical parity difference\",\n",
        "                                    metric_ub=0.05, metric_lb=-0.05)\n",
        "    ROC = ROC.fit(data, data_pred)\n",
        "    # dataset_transf.append(ROC.predict(data))\n",
        "\n",
        "    # dataset_transf.append(ROC.fit_transform(data, pred))\n",
        "\n",
        "    return ROC.predict(data)\n",
        "\n",
        "def Cal_EqOdds(data, data_pred):\n",
        "  # dataset_transf = []\n",
        "  # for data, data_pred in zip(datas,datapreds):\n",
        "    cpp = CalibratedEqOddsPostprocessing(privileged_groups = privileged_groups,\n",
        "                                     unprivileged_groups = unprivileged_groups,\n",
        "                                     cost_constraint='weighted'\n",
        "                                     )\n",
        "    cpp = cpp.fit(data, data_pred)\n",
        "    # dataset_transf.append(cpp.predict(data_pred))\n",
        "\n",
        "    return cpp.predict(data_pred)\n",
        "\n",
        "'''\n",
        "\n",
        "        From Eqodds postprocessing library source file:\n",
        "\n",
        "        metric = ClassificationMetric(dataset_true, dataset_pred,\n",
        "            unprivileged_groups=self.unprivileged_groups,\n",
        "            privileged_groups=self.privileged_groups)\n",
        "\n",
        "        # compute basic statistics\n",
        "        sbr = metric.base_rate(privileged=True)\n",
        "        obr = metric.base_rate(privileged=False)\n",
        "\n",
        "        fpr0 = metric.false_positive_rate(privileged=True)\n",
        "        fpr1 = metric.false_positive_rate(privileged=False)\n",
        "        fnr0 = metric.false_negative_rate(privileged=True)\n",
        "        fnr1 = metric.false_negative_rate(privileged=False)\n",
        "        tpr0 = metric.true_positive_rate(privileged=True)\n",
        "        tpr1 = metric.true_positive_rate(privileged=False)\n",
        "        tnr0 = metric.true_negative_rate(privileged=True)\n",
        "        tnr1 = metric.true_negative_rate(privileged=False)\n",
        "\n",
        "        # linear program has 4 decision variables:\n",
        "        # [Pr[label_tilde = 1 | label_hat = 1, protected_attributes = 0];\n",
        "        #  Pr[label_tilde = 1 | label_hat = 0, protected_attributes = 0];\n",
        "        #  Pr[label_tilde = 1 | label_hat = 1, protected_attributes = 1];\n",
        "        #  Pr[label_tilde = 1 | label_hat = 0, protected_attributes = 1]]\n",
        "        # Coefficients of the linear objective function to be minimized.\n",
        "        c = np.array([fpr0 - tpr0, tnr0 - fnr0, fpr1 - tpr1, tnr1 - fnr1])\n",
        "\n",
        "\n",
        "        Error on executing below function\n",
        "        ValueError: Invalid input for linprog: c must not contain values inf, nan, or None\n",
        "        '''\n",
        "\n",
        "# def EqOdds(datas, datapreds):\n",
        "#   dataset_transf = []\n",
        "#   for data, data_pred in zip(datas,datapreds):\n",
        "#     epp = EqOddsPostprocessing(unprivileged_groups=unprivileged_groups, privileged_groups=privileged_groups, seed=None)\n",
        "#     epp = epp.fit(data, data_pred)\n",
        "#     dataset_transf.append(epp.predict(data_pred))\n",
        "\n",
        "#   return dataset_transf\n",
        "\n",
        "\n",
        "def EqOdds(datas, datapreds):\n",
        "  return datapreds\n",
        "\n",
        "\n",
        "# # replaced with cal_metric function\n",
        "# def evaluate_performance(post_processed_data, labels):\n",
        "#   pass\n",
        "\n",
        "# empty function for original data\n",
        "def pre_no_fn(ls):\n",
        "  # train = ls[0]\n",
        "  # valid = ls[1]\n",
        "  # test = ls[2]\n",
        "  # return train,valid,test\n",
        "  return ls\n",
        "\n",
        "def in_no_fn(ls2):\n",
        "  # train = ls[0]\n",
        "  # valid = ls[1]\n",
        "  # test = ls[2]\n",
        "  # return train,valid,test\n",
        "  return ls2\n",
        "\n",
        "def post_no_fn(ls, ls2):\n",
        "  # train = ls[0]\n",
        "  # valid = ls[1]\n",
        "  # test = ls[2]\n",
        "  # return train,valid,test\n",
        "  return ls2\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CSeCmgGh1id5"
      },
      "outputs": [],
      "source": [
        "# # retest rejopt function with fit_transform\n",
        "\n",
        "# if debug == 1:\n",
        "#     train_pred, valid_pred, test_pred, best_class_thresh = pred_preproc_fit_model(dataset_orig_train, dataset_orig_valid, dataset_orig_test)\n",
        "#     ls = EqOdds([dataset_orig_train, dataset_orig_valid, dataset_orig_test],[train_pred, valid_pred, test_pred])\n",
        "#     print(ls)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eKs7eLbK1id6"
      },
      "outputs": [],
      "source": [
        "# if debug == 1:\n",
        "#     ls = adversial_debiasing([dataset_orig_train, dataset_orig_valid, dataset_orig_test])\n",
        "#     ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2rJqNaw31id6"
      },
      "outputs": [],
      "source": [
        "# if debug == 1:\n",
        "#   dataset_orig_train.features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qmt6o8mL1id6"
      },
      "outputs": [],
      "source": [
        "def cal_metric(test, split_data_pred, best_class_thresh):\n",
        "        fav_inds = split_data_pred.scores > best_class_thresh\n",
        "        split_data_pred.labels[fav_inds] = split_data_pred.favorable_label\n",
        "        split_data_pred.labels[~fav_inds] = split_data_pred.unfavorable_label\n",
        "\n",
        "\n",
        "\n",
        "        metric_obj = ClassificationMetric(test, split_data_pred,\n",
        "                                        unprivileged_groups, privileged_groups)\n",
        "\n",
        "        metric = []\n",
        "\n",
        "        metric.append(abs(metric_obj.mean_difference()))\n",
        "        metric.append(abs(1-metric_obj.disparate_impact()))\n",
        "        metric.append(abs(metric_obj.theil_index()))\n",
        "\n",
        "        return sum(metric)/len(metric)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the preprocessing algorithms\n",
        "preprocessing_algos = {\n",
        "        'pre1': reweighing,\n",
        "        'pre2': disparate_impact_remover,\n",
        "        'pre3': lfr,\n",
        "        'pre4': optim_preproc,\n",
        "        'pre_none': pre_no_fn                         # empty function\n",
        "}\n",
        "\n",
        "# Define the in-processing algorithms\n",
        "in_processing_algos = {\n",
        "        'in1': exp_grad_red_in_proc,\n",
        "        'in2': gerry_in_proc,\n",
        "        'in3': prejudice_remover,\n",
        "        # 'in4': adversarial_debiasing_in_proc,       # no none value in test data but error occurs\n",
        "        'in_none': in_no_fn\n",
        "}\n",
        "\n",
        "# Define the post-processing algorithms\n",
        "post_processing_algos = {\n",
        "        'post1': RejOpt,\n",
        "        'post2': Cal_EqOdds,\n",
        "        # 'post3': EqOdds,                            # facing issue, empty function\n",
        "        'post_none': post_no_fn\n",
        "}\n"
      ],
      "metadata": {
        "id": "z2LQ1R1BSpcD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z_pGYQPD1id6"
      },
      "outputs": [],
      "source": [
        "from copy import deepcopy\n",
        "from itertools import product\n",
        "\n",
        "def test_preprocessing_combinations(train,valid,test):\n",
        "\n",
        "    # Store the scores for each combination\n",
        "    combination_scores = {}\n",
        "\n",
        "    # Generate all possible combinations of preprocessing, in-processing, and post-processing algorithms\n",
        "    all_combinations = product(preprocessing_algos.keys(),\n",
        "                               in_processing_algos.keys(),\n",
        "                               post_processing_algos.keys())\n",
        "\n",
        "    preproc_data = {}\n",
        "    inproc_data = {}\n",
        "\n",
        "    # Test each combination and store the scores\n",
        "    for combination in all_combinations:\n",
        "        print(combination)\n",
        "\n",
        "        pre_algo, in_algo, post_algo = combination\n",
        "        # pre_algo, post_algo = combination\n",
        "\n",
        "        # train = og_train.copy(deepcopy=True)\n",
        "        # valid = og_valid.copy(deepcopy=True)\n",
        "        # test = og_test.copy(deepcopy=True)\n",
        "\n",
        "\n",
        "        try:\n",
        "        #   preprocessed_data = preproc_data[pre_algo]\n",
        "            ls = preproc_data[pre_algo]\n",
        "            # pre_train = ls[0]\n",
        "            # pre_valid = ls[1]\n",
        "            # pre_test = ls[2]\n",
        "\n",
        "            # optimizing space comp\n",
        "            trf_train = ls[0]\n",
        "            trf_valid = ls[1]\n",
        "            trf_test = ls[2]\n",
        "\n",
        "        except:\n",
        "          # Apply the preprocessing algorithm\n",
        "          # preprocessed_data = preprocessing_algos[pre_algo](train, valid, test)\n",
        "            [trf_train, trf_valid, trf_test] = preprocessing_algos[pre_algo]([train, valid, test])\n",
        "            # preproc_data[pre_algo] = preprocessed_data\n",
        "            preproc_data[pre_algo] = [trf_train, trf_valid, trf_test]\n",
        "\n",
        "        if debug ==1:\n",
        "            # if og_train == train:\n",
        "            #     print(\"train not modif\")\n",
        "            print(trf_train, trf_valid, trf_test)\n",
        "            # check_pt = pre_train.copy(deepcopy=True)\n",
        "            # check_pv = pre_valid.copy(deepcopy=True)\n",
        "            # check_ptt = pre_test.copy(deepcopy=True)\n",
        "\n",
        "\n",
        "\n",
        "        train_pred, valid_pred, test_pred, best_class_thresh = pred_preproc_fit_model(trf_train, trf_valid, trf_test)\n",
        "\n",
        "        if debug==1:\n",
        "            # if check_pt == pre_train:\n",
        "                # print(\"pre train not modif\")\n",
        "            print(\"pred proc OK\")\n",
        "\n",
        "            # print(train_pred, valid_pred, test_pred, best_class_thresh)\n",
        "            # print(train_pred)\n",
        "            # print(valid_pred)\n",
        "            print(\"test: \\n\", test)\n",
        "            print(\"test_pred: \\n\", test_pred)\n",
        "            # print(best_class_thresh)\n",
        "\n",
        "        # Apply the in-processing algorithm\n",
        "        # print(StructuredDataset.convert_to_dataframe(trf_test)[0].isnull().any())\n",
        "        try:\n",
        "          test_pred2 = inproc_data[str(pre_algo)+str(in_algo)]\n",
        "        except:\n",
        "          # test_pred2 = in_processing_algos[in_algo](trf_train, trf_test)\n",
        "          test_pred2 = in_processing_algos[in_algo](trf_test)\n",
        "          inproc_data[str(pre_algo)+str(in_algo)] = test_pred2\n",
        "\n",
        "        if test_pred2!=trf_test:\n",
        "          test_pred = test_pred2\n",
        "\n",
        "        # Apply the post-processing algorithm\n",
        "        # [trf_train, trf_valid, trf_test] = post_processing_algos[post_algo]([trf_train, trf_valid, trf_test], [train_pred, valid_pred, test_pred])\n",
        "        trf_test = post_processing_algos[post_algo](trf_test, test_pred)\n",
        "\n",
        "        # Evaluate the performance and store the score\n",
        "        # score = evaluate_performance(post_processed_data, labels)\n",
        "\n",
        "        # # removing labels from test pred for metric function\n",
        "        # temp_label = test_pred.labels\n",
        "        # del test_pred.labels\n",
        "        # # test_pred.labels = None\n",
        "        # test2 = test_pred.copy(deepcopy=True)\n",
        "        # test_pred.labels = temp_label\n",
        "\n",
        "        # # removing labels try 2\n",
        "        # temp_label = test_pred.labels\n",
        "        # del test_pred.features['labels']  # Remove the 'labels' column from the features\n",
        "        # test2 = test_pred.copy(deepcopy=True)\n",
        "        # # Assign the original labels back to the modified dataset\n",
        "        # test_pred.features['labels'] = temp_label\n",
        "\n",
        "        # test_pred = BinaryLabelDataset(\n",
        "        #     favorable_label=1,\n",
        "        #     unfavorable_label=0,\n",
        "        #     df=test_pred,\n",
        "        #     label_names = test_pred.labels,\n",
        "        #     protected_attribute_names = ['sex']\n",
        "        # )\n",
        "\n",
        "        # split_data_pred = BinaryLabelDataset(\n",
        "        #     favorable_label=1,\n",
        "        #     unfavorable_label=0,\n",
        "        #     df=split_data_pred,\n",
        "        #     label_names = split_data_pred.labels,\n",
        "        #     protected_attribute_names = ['sex']\n",
        "        # )\n",
        "\n",
        "\n",
        "        score = cal_metric(trf_test, test_pred, best_class_thresh)\n",
        "        combination_scores[combination] = score\n",
        "\n",
        "\n",
        "    # Select the combination with the best score --> 0\n",
        "    best_combination = min(combination_scores, key=combination_scores.get)\n",
        "    best_score = combination_scores[best_combination]\n",
        "\n",
        "    return best_combination, best_score\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "debug = 0"
      ],
      "metadata": {
        "id": "7gx9fxy5IQsR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "psayCBWy1id7",
        "outputId": "a53eeb90-242d-4d9f-d91d-c117da9c0e87"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('pre1', 'in1', 'post1')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('pre1', 'in1', 'post2')\n",
            "('pre1', 'in1', 'post_none')\n",
            "('pre1', 'in2', 'post1')\n",
            "iteration: 1, error: 0.20349392657294937, fairness violation: 0.015291740163445939, violated group size: 0.2908420908966835\n",
            "iteration: 2, error: 0.22512624539374915, fairness violation: 0.0076458700817229695, violated group size: 0.2908420908966835\n",
            "iteration: 3, error: 0.23233701833401577, fairness violation: 0.005097246721148648, violated group size: 0.46239934488876755\n",
            "iteration: 4, error: 0.23594240480414905, fairness violation: 0.0038229350408614847, violated group size: 0.2908420908966835\n",
            "iteration: 5, error: 0.238105636686229, fairness violation: 0.0030583480326891897, violated group size: 0.46239934488876755\n",
            "iteration: 6, error: 0.2395477912742823, fairness violation: 0.0025486233605743237, violated group size: 0.2908420908966835\n",
            "iteration: 7, error: 0.24057790169432042, fairness violation: 0.002184534309063706, violated group size: 0.2908420908966835\n",
            "iteration: 8, error: 0.24135048450934898, fairness violation: 0.0019114675204307432, violated group size: 0.2908420908966835\n",
            "iteration: 9, error: 0.24195138225437118, fairness violation: 0.0016990822403828833, violated group size: 0.46239934488876755\n",
            "iteration: 10, error: 0.242432100450389, fairness violation: 0.0015291740163445944, violated group size: 0.2908420908966835\n",
            "iteration: 11, error: 0.2428254153380399, fairness violation: 0.0013901581966769042, violated group size: 0.46239934488876755\n",
            "iteration: 12, error: 0.24204995223147263, fairness violation: 0.0014719304229692716, violated group size: 0.2908420908966835\n",
            "iteration: 13, error: 0.24139379114130036, fairness violation: 0.0015411223067551211, violated group size: 0.2908420908966835\n",
            "iteration: 14, error: 0.2408313673497241, fairness violation: 0.0016004296357144216, violated group size: 0.46239934488876755\n",
            "iteration: 15, error: 0.24034393339702473, fairness violation: 0.001651829320812481, violated group size: 0.2908420908966835\n",
            "iteration: 16, error: 0.23991742868841273, fairness violation: 0.001696804045273283, violated group size: 0.2908420908966835\n",
            "iteration: 17, error: 0.23942870446936795, fairness violation: 0.0017488872330638488, violated group size: 0.46239934488876755\n",
            "iteration: 18, error: 0.2389942829413281, fairness violation: 0.001795183399988796, violated group size: 0.2908420908966835\n",
            "iteration: 19, error: 0.23860558999518722, fairness violation: 0.0018366062861848015, violated group size: 0.46239934488876755\n",
            "iteration: 20, error: 0.23732769209772078, fairness violation: 0.0021663626229307285, violated group size: 0.46239934488876755\n",
            "iteration: 21, error: 0.2361714987619178, fairness violation: 0.0024647135942722814, violated group size: 0.46239934488876755\n",
            "iteration: 22, error: 0.23512041391118776, fairness violation: 0.00273594175003733, violated group size: 0.46239934488876755\n",
            "iteration: 23, error: 0.23416072774312993, fairness violation: 0.0029835848487793307, violated group size: 0.2908420908966835\n",
            "iteration: 24, error: 0.23328101542241025, fairness violation: 0.0032105910226261644, violated group size: 0.2908420908966835\n",
            "iteration: 25, error: 0.23247168008734814, fairness violation: 0.003419436702565252, violated group size: 0.46239934488876755\n",
            "iteration: 26, error: 0.23172460131652162, fairness violation: 0.003612217330201334, violated group size: 0.46239934488876755\n",
            "iteration: 27, error: 0.23103286171390444, fairness violation: 0.0037907179113458515, violated group size: 0.2908420908966835\n",
            "iteration: 28, error: 0.23039053208290278, fairness violation: 0.003956468450980047, violated group size: 0.2908420908966835\n",
            "iteration: 29, error: 0.2297925010471426, fairness violation: 0.004110787918915334, violated group size: 0.46239934488876755\n",
            "iteration: 30, error: 0.22923433874709978, fairness violation: 0.0042548194223216, violated group size: 0.2908420908966835\n",
            "iteration: 31, error: 0.22871218691802747, fairness violation: 0.004389558570669399, violated group size: 0.46239934488876755\n",
            "iteration: 32, error: 0.22822266957827214, fairness violation: 0.00451587652224546, violated group size: 0.2908420908966835\n",
            "iteration: 33, error: 0.22772559772696255, fairness violation: 0.004656895708251654, violated group size: 0.2908420908966835\n",
            "iteration: 34, error: 0.22725776539631823, fairness violation: 0.004789619648022191, violated group size: 0.2908420908966835\n",
            "iteration: 35, error: 0.22681666634171072, fairness violation: 0.0049147593626629816, violated group size: 0.2908420908966835\n",
            "iteration: 36, error: 0.22640007279013694, fairness violation: 0.005032946870934841, violated group size: 0.46239934488876755\n",
            "iteration: 37, error: 0.22603919601326453, fairness violation: 0.00512480595607453, violated group size: 0.46239934488876755\n",
            "iteration: 38, error: 0.22569731275096436, fairness violation: 0.005211830352522657, violated group size: 0.2908420908966835\n",
            "iteration: 39, error: 0.22537296196365394, fairness violation: 0.005294391959409343, violated group size: 0.46239934488876755\n",
            "iteration: 40, error: 0.22552886583867887, fairness violation: 0.005226587616366929, violated group size: 0.2908420908966835\n",
            "iteration: 41, error: 0.22572376808796069, fairness violation: 0.0051569495016307925, violated group size: 0.2908420908966835\n",
            "iteration: 42, error: 0.2259093892777529, fairness violation: 0.005090627487596378, violated group size: 0.2908420908966835\n",
            "iteration: 43, error: 0.22608637692383382, fairness violation: 0.0050273902184007685, violated group size: 0.2908420908966835\n",
            "iteration: 44, error: 0.2262553196769111, fairness violation: 0.0049670273705322366, violated group size: 0.2908420908966835\n",
            "iteration: 45, error: 0.22641675386318486, fairness violation: 0.004909347315902305, violated group size: 0.46239934488876755\n",
            "iteration: 46, error: 0.22657116917179465, fairness violation: 0.004854175089734544, violated group size: 0.2908420908966835\n",
            "iteration: 47, error: 0.2267190136162082, fairness violation: 0.004801350617871795, violated group size: 0.2908420908966835\n",
            "iteration: 48, error: 0.2268606978754379, fairness violation: 0.004750727165669994, violated group size: 0.2908420908966835\n",
            "iteration: 49, error: 0.2269965991036786, fairness violation: 0.004702169976823368, violated group size: 0.46239934488876755\n",
            "('pre1', 'in2', 'post2')\n",
            "('pre1', 'in2', 'post_none')\n",
            "('pre1', 'in3', 'post1')\n",
            "('pre1', 'in3', 'post2')\n",
            "('pre1', 'in3', 'post_none')\n",
            "('pre1', 'in_none', 'post1')\n",
            "('pre1', 'in_none', 'post2')\n",
            "('pre1', 'in_none', 'post_none')\n",
            "('pre2', 'in1', 'post1')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('pre2', 'in1', 'post2')\n",
            "('pre2', 'in1', 'post_none')\n",
            "('pre2', 'in2', 'post1')\n",
            "iteration: 1, error: 0.20349392657294937, fairness violation: 0.015291740163445939, violated group size: 0.2908420908966835\n",
            "iteration: 2, error: 0.22512624539374915, fairness violation: 0.0076458700817229695, violated group size: 0.2908420908966835\n",
            "iteration: 3, error: 0.23233701833401577, fairness violation: 0.005097246721148648, violated group size: 0.46239934488876755\n",
            "iteration: 4, error: 0.23594240480414905, fairness violation: 0.0038229350408614847, violated group size: 0.2908420908966835\n",
            "iteration: 5, error: 0.238105636686229, fairness violation: 0.0030583480326891897, violated group size: 0.46239934488876755\n",
            "iteration: 6, error: 0.2395477912742823, fairness violation: 0.0025486233605743237, violated group size: 0.2908420908966835\n",
            "iteration: 7, error: 0.24057790169432042, fairness violation: 0.002184534309063706, violated group size: 0.2908420908966835\n",
            "iteration: 8, error: 0.24135048450934898, fairness violation: 0.0019114675204307432, violated group size: 0.2908420908966835\n",
            "iteration: 9, error: 0.24195138225437118, fairness violation: 0.0016990822403828833, violated group size: 0.46239934488876755\n",
            "iteration: 10, error: 0.242432100450389, fairness violation: 0.0015291740163445944, violated group size: 0.2908420908966835\n",
            "iteration: 11, error: 0.2428254153380399, fairness violation: 0.0013901581966769042, violated group size: 0.46239934488876755\n",
            "iteration: 12, error: 0.24204995223147263, fairness violation: 0.0014719304229692716, violated group size: 0.2908420908966835\n",
            "iteration: 13, error: 0.24139379114130036, fairness violation: 0.0015411223067551211, violated group size: 0.2908420908966835\n",
            "iteration: 14, error: 0.2408313673497241, fairness violation: 0.0016004296357144216, violated group size: 0.46239934488876755\n",
            "iteration: 15, error: 0.24034393339702473, fairness violation: 0.001651829320812481, violated group size: 0.2908420908966835\n",
            "iteration: 16, error: 0.23991742868841273, fairness violation: 0.001696804045273283, violated group size: 0.2908420908966835\n",
            "iteration: 17, error: 0.23942870446936795, fairness violation: 0.0017488872330638488, violated group size: 0.46239934488876755\n",
            "iteration: 18, error: 0.2389942829413281, fairness violation: 0.001795183399988796, violated group size: 0.2908420908966835\n",
            "iteration: 19, error: 0.23860558999518722, fairness violation: 0.0018366062861848015, violated group size: 0.46239934488876755\n",
            "iteration: 20, error: 0.23732769209772078, fairness violation: 0.0021663626229307285, violated group size: 0.46239934488876755\n",
            "iteration: 21, error: 0.2361714987619178, fairness violation: 0.0024647135942722814, violated group size: 0.46239934488876755\n",
            "iteration: 22, error: 0.23512041391118776, fairness violation: 0.00273594175003733, violated group size: 0.46239934488876755\n",
            "iteration: 23, error: 0.23416072774312993, fairness violation: 0.0029835848487793307, violated group size: 0.2908420908966835\n",
            "iteration: 24, error: 0.23328101542241025, fairness violation: 0.0032105910226261644, violated group size: 0.2908420908966835\n",
            "iteration: 25, error: 0.23247168008734814, fairness violation: 0.003419436702565252, violated group size: 0.46239934488876755\n",
            "iteration: 26, error: 0.23172460131652162, fairness violation: 0.003612217330201334, violated group size: 0.46239934488876755\n",
            "iteration: 27, error: 0.23103286171390444, fairness violation: 0.0037907179113458515, violated group size: 0.2908420908966835\n",
            "iteration: 28, error: 0.23039053208290278, fairness violation: 0.003956468450980047, violated group size: 0.2908420908966835\n",
            "iteration: 29, error: 0.2297925010471426, fairness violation: 0.004110787918915334, violated group size: 0.46239934488876755\n",
            "iteration: 30, error: 0.22923433874709978, fairness violation: 0.0042548194223216, violated group size: 0.2908420908966835\n",
            "iteration: 31, error: 0.22871218691802747, fairness violation: 0.004389558570669399, violated group size: 0.46239934488876755\n",
            "iteration: 32, error: 0.22822266957827214, fairness violation: 0.00451587652224546, violated group size: 0.2908420908966835\n",
            "iteration: 33, error: 0.22772559772696255, fairness violation: 0.004656895708251654, violated group size: 0.2908420908966835\n",
            "iteration: 34, error: 0.22725776539631823, fairness violation: 0.004789619648022191, violated group size: 0.2908420908966835\n",
            "iteration: 35, error: 0.22681666634171072, fairness violation: 0.0049147593626629816, violated group size: 0.2908420908966835\n",
            "iteration: 36, error: 0.22640007279013694, fairness violation: 0.005032946870934841, violated group size: 0.46239934488876755\n",
            "iteration: 37, error: 0.22603919601326453, fairness violation: 0.00512480595607453, violated group size: 0.46239934488876755\n",
            "iteration: 38, error: 0.22569731275096436, fairness violation: 0.005211830352522657, violated group size: 0.2908420908966835\n",
            "iteration: 39, error: 0.22537296196365394, fairness violation: 0.005294391959409343, violated group size: 0.46239934488876755\n",
            "iteration: 40, error: 0.22552886583867887, fairness violation: 0.005226587616366929, violated group size: 0.2908420908966835\n",
            "iteration: 41, error: 0.22572376808796069, fairness violation: 0.0051569495016307925, violated group size: 0.2908420908966835\n",
            "iteration: 42, error: 0.2259093892777529, fairness violation: 0.005090627487596378, violated group size: 0.2908420908966835\n",
            "iteration: 43, error: 0.22608637692383382, fairness violation: 0.0050273902184007685, violated group size: 0.2908420908966835\n",
            "iteration: 44, error: 0.2262553196769111, fairness violation: 0.0049670273705322366, violated group size: 0.2908420908966835\n",
            "iteration: 45, error: 0.22641675386318486, fairness violation: 0.004909347315902305, violated group size: 0.46239934488876755\n",
            "iteration: 46, error: 0.22657116917179465, fairness violation: 0.004854175089734544, violated group size: 0.2908420908966835\n",
            "iteration: 47, error: 0.2267190136162082, fairness violation: 0.004801350617871795, violated group size: 0.2908420908966835\n",
            "iteration: 48, error: 0.2268606978754379, fairness violation: 0.004750727165669994, violated group size: 0.2908420908966835\n",
            "iteration: 49, error: 0.2269965991036786, fairness violation: 0.004702169976823368, violated group size: 0.46239934488876755\n",
            "('pre2', 'in2', 'post2')\n",
            "('pre2', 'in2', 'post_none')\n",
            "('pre2', 'in3', 'post1')\n",
            "('pre2', 'in3', 'post2')\n",
            "('pre2', 'in3', 'post_none')\n",
            "('pre2', 'in_none', 'post1')\n",
            "('pre2', 'in_none', 'post2')\n",
            "('pre2', 'in_none', 'post_none')\n",
            "('pre3', 'in1', 'post1')\n",
            "step: 0, loss: 0.8520385225546282, L_x: 0.410404621212644,  L_y: 0.7924638799282414,  L_z: 0.009267090252561163\n",
            "step: 250, loss: 0.7000338460974855, L_x: 0.40840994139357334,  L_y: 0.6413614165428891,  L_z: 0.008915717707619489\n",
            "step: 500, loss: 0.6060816690182961, L_x: 0.4035320750563819,  L_y: 0.5506960175591624,  L_z: 0.007516221976747712\n",
            "step: 750, loss: 0.5948445975849865, L_x: 0.39957344653321136,  L_y: 0.5427640369918133,  L_z: 0.006061607969926046\n",
            "step: 1000, loss: 0.5918290901504581, L_x: 0.3667518382913424,  L_y: 0.5399586074059991,  L_z: 0.007597649457662376\n",
            "step: 1250, loss: 0.5829040411496046, L_x: 0.3840632986751374,  L_y: 0.5392185966891572,  L_z: 0.0026395572964668422\n",
            "step: 1500, loss: 0.5757032196290076, L_x: 0.3788269707666351,  L_y: 0.5334262155950279,  L_z: 0.002197153478658133\n",
            "step: 1750, loss: 0.5616186302684035, L_x: 0.34367193994903655,  L_y: 0.516620467739262,  L_z: 0.005315484267118957\n",
            "step: 2000, loss: 0.5440580709420663, L_x: 0.297980546344582,  L_y: 0.5001177015188928,  L_z: 0.007071157394357669\n",
            "step: 2250, loss: 0.5356865410792271, L_x: 0.2529495653510789,  L_y: 0.49081396050271003,  L_z: 0.009788812020704576\n",
            "step: 2500, loss: 0.5233986577300037, L_x: 0.23556385046117215,  L_y: 0.4863326830160134,  L_z: 0.006754794833936563\n",
            "step: 2750, loss: 0.5196385063296517, L_x: 0.23425170238662868,  L_y: 0.48495331812672393,  L_z: 0.005630008982132439\n",
            "step: 3000, loss: 0.5172652004556797, L_x: 0.228912422294204,  L_y: 0.4815205427405159,  L_z: 0.006426707742871673\n",
            "step: 3250, loss: 0.5086289495094733, L_x: 0.212250157241442,  L_y: 0.47528310970006526,  L_z: 0.006060412042631953\n",
            "step: 3500, loss: 0.5062369554513934, L_x: 0.2092030524646578,  L_y: 0.4734497127635621,  L_z: 0.005933468720682752\n",
            "step: 3750, loss: 0.504463338371301, L_x: 0.20759789179736524,  L_y: 0.47291056394279607,  L_z: 0.005396492624384163\n",
            "step: 4000, loss: 0.5038062293563358, L_x: 0.20418337350788962,  L_y: 0.4717220001343826,  L_z: 0.005832945935582125\n",
            "step: 4250, loss: 0.5011162314393269, L_x: 0.20510842916301977,  L_y: 0.47148866465067135,  L_z: 0.004558361936176835\n",
            "step: 4500, loss: 0.5007556558751559, L_x: 0.20486841823632881,  L_y: 0.4713090315576871,  L_z: 0.0044798912469179615\n",
            "step: 4750, loss: 0.4995318973710403, L_x: 0.2044380660985553,  L_y: 0.4713039120589014,  L_z: 0.0038920893511417212\n",
            "step: 5000, loss: 0.49961220164373743, L_x: 0.20415779593795758,  L_y: 0.4716506813418106,  L_z: 0.0037728703540655306\n",
            "step: 5250, loss: 0.49949803063830855, L_x: 0.20432523209399983,  L_y: 0.47138042073343345,  L_z: 0.003842543347737566\n",
            "step: 0, loss: 0.8881696318828007, L_x: 0.4973863300646719,  L_y: 0.8095156169586085,  L_z: 0.01445769095886249\n",
            "step: 250, loss: 0.713107934456011, L_x: 0.4982051890245601,  L_y: 0.6351126638008505,  L_z: 0.014087375876352237\n",
            "step: 500, loss: 0.6269867320013065, L_x: 0.49582921591447193,  L_y: 0.5511810188324384,  L_z: 0.013111395788710447\n",
            "step: 750, loss: 0.6177606685502545, L_x: 0.49155852128256045,  L_y: 0.5445019229205585,  L_z: 0.01205144675071996\n",
            "step: 1000, loss: 0.6063577043448177, L_x: 0.4777108363565633,  L_y: 0.5399990622107722,  L_z: 0.00929377924919461\n",
            "step: 1250, loss: 0.5971966112086227, L_x: 0.33854989262425084,  L_y: 0.5223365194643854,  L_z: 0.020502551240906063\n",
            "step: 1500, loss: 0.5656809597825113, L_x: 0.313167684291071,  L_y: 0.4982765805776394,  L_z: 0.018043805387882417\n",
            "step: 1750, loss: 0.5359757729878827, L_x: 0.32465945164538745,  L_y: 0.48608849463415427,  L_z: 0.008710666594594858\n",
            "step: 2000, loss: 0.5272353244816496, L_x: 0.29390293601348905,  L_y: 0.47877070236663005,  L_z: 0.009537164256835307\n",
            "step: 2250, loss: 0.5215003768448747, L_x: 0.2516655916229926,  L_y: 0.4744767112361253,  L_z: 0.01092855322322506\n",
            "step: 2500, loss: 0.5097772432407601, L_x: 0.22709768704428857,  L_y: 0.46833506076683734,  L_z: 0.009366206884746969\n",
            "step: 2750, loss: 0.5076245640649989, L_x: 0.22133914195190246,  L_y: 0.46808841048751804,  L_z: 0.008701119691145329\n",
            "step: 3000, loss: 0.5064922650045491, L_x: 0.21896345517793947,  L_y: 0.4683133089061574,  L_z: 0.008141305290298911\n",
            "step: 3250, loss: 0.5017493540834954, L_x: 0.21298342958362737,  L_y: 0.4674354084060318,  L_z: 0.006507801359550451\n",
            "step: 3500, loss: 0.5054883338484345, L_x: 0.20690053097307418,  L_y: 0.46894858223179087,  L_z: 0.007924849259668118\n",
            "step: 3750, loss: 0.5012910289319937, L_x: 0.21121863460847518,  L_y: 0.46704400045952704,  L_z: 0.006562582505809591\n",
            "step: 4000, loss: 0.5003574161702857, L_x: 0.21056615139761098,  L_y: 0.4669490438293622,  L_z: 0.006175878600581224\n",
            "step: 4250, loss: 0.5000953287160302, L_x: 0.20995847780286853,  L_y: 0.467024922503078,  L_z: 0.006037279216332731\n",
            "step: 4500, loss: 0.502370268266402, L_x: 0.2063850383881071,  L_y: 0.46816520872604334,  L_z: 0.006783277850773936\n",
            "step: 4750, loss: 0.5001583113480309, L_x: 0.20925468841558975,  L_y: 0.4671574904953179,  L_z: 0.006037676005577023\n",
            "step: 5000, loss: 0.5000931505755718, L_x: 0.2094037862928407,  L_y: 0.46712693782345943,  L_z: 0.006012917061414184\n",
            "step: 5250, loss: 0.4999896468307393, L_x: 0.2096516126959972,  L_y: 0.46707917336957777,  L_z: 0.005972656095780894\n",
            "step: 5500, loss: 0.49998406797079514, L_x: 0.20967659408579048,  L_y: 0.4670745654310541,  L_z: 0.005970921565581008\n",
            "step: 0, loss: 0.9078318153705022, L_x: 0.49188100127661893,  L_y: 0.8383467476619973,  L_z: 0.010148483790421513\n",
            "step: 250, loss: 0.7137792349188148, L_x: 0.4913274649396088,  L_y: 0.6450770679660778,  L_z: 0.009784710229388054\n",
            "step: 500, loss: 0.6275277488823622, L_x: 0.4863687786336572,  L_y: 0.5617285667077022,  L_z: 0.008581152155647141\n",
            "step: 750, loss: 0.6234053574206921, L_x: 0.4825295984153618,  L_y: 0.5600667442969253,  L_z: 0.007542826641115356\n",
            "step: 1000, loss: 0.6084485649970881, L_x: 0.4597195267736315,  L_y: 0.554941853030595,  L_z: 0.0037673796445649465\n",
            "step: 1250, loss: 0.5976376107403037, L_x: 0.4364390704118042,  L_y: 0.5483492852513547,  L_z: 0.0028222092238843104\n",
            "step: 1500, loss: 0.5929748832575598, L_x: 0.39650393614005564,  L_y: 0.5427321844748247,  L_z: 0.0052961525843647035\n",
            "step: 1750, loss: 0.5831330160473057, L_x: 0.3654904043621684,  L_y: 0.5354483292376355,  L_z: 0.005567823186726641\n",
            "step: 2000, loss: 0.561057761490747, L_x: 0.3166520491447877,  L_y: 0.5186042965712497,  L_z: 0.005394130002509252\n",
            "step: 2250, loss: 0.5482527536110339, L_x: 0.2782525923857403,  L_y: 0.505820085059767,  L_z: 0.007303704656346409\n",
            "step: 2500, loss: 0.5285006785173186, L_x: 0.23057609971747067,  L_y: 0.4911247409097426,  L_z: 0.007159163817914496\n",
            "step: 2750, loss: 0.538323798666515, L_x: 0.20768649272333434,  L_y: 0.48761077835976613,  L_z: 0.014972185517207753\n",
            "step: 3000, loss: 0.5249406684169993, L_x: 0.21656747764338186,  L_y: 0.48662625513553737,  L_z: 0.008328832758561819\n",
            "step: 3250, loss: 0.5183999424685145, L_x: 0.20721192762173157,  L_y: 0.48237600715544404,  L_z: 0.007651371275448626\n",
            "step: 3500, loss: 0.51463035702992, L_x: 0.20347410271251923,  L_y: 0.4805497699800672,  L_z: 0.006866588389300423\n",
            "step: 3750, loss: 0.5228344050372767, L_x: 0.22015000907244103,  L_y: 0.47403771067966194,  L_z: 0.013390846725185358\n",
            "step: 4000, loss: 0.5129030998036775, L_x: 0.19639906534119944,  L_y: 0.477427154150157,  L_z: 0.007918019559700267\n",
            "step: 4250, loss: 0.5162267484930029, L_x: 0.20140917151286303,  L_y: 0.47879874672890926,  L_z: 0.00864354230640366\n",
            "step: 4500, loss: 0.5081961552441966, L_x: 0.199578956094521,  L_y: 0.4766559015683425,  L_z: 0.0057911790332010205\n",
            "step: 4750, loss: 0.507588080288461, L_x: 0.20034378922682414,  L_y: 0.4765112941212431,  L_z: 0.005521203622267787\n",
            "step: 5000, loss: 0.5059568406262904, L_x: 0.20126961399512402,  L_y: 0.4761996051190618,  L_z: 0.004815137053858125\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('pre3', 'in1', 'post2')\n",
            "('pre3', 'in1', 'post_none')\n",
            "('pre3', 'in2', 'post1')\n",
            "iteration: 1, error: 0.0005459260270233384, fairness violation: 0.00036067903250839385, violated group size: 0.3065374641736045\n",
            "iteration: 2, error: 0.0005459260270233384, fairness violation: 0.00036067903250839385, violated group size: 0.3065374641736045\n",
            "iteration: 3, error: 0.0005459260270233384, fairness violation: 0.00036067903250839385, violated group size: 0.3065374641736045\n",
            "iteration: 4, error: 0.0005459260270233384, fairness violation: 0.00036067903250839385, violated group size: 0.3065374641736045\n",
            "iteration: 5, error: 0.0005459260270233384, fairness violation: 0.00036067903250839385, violated group size: 0.3065374641736045\n",
            "('pre3', 'in2', 'post2')\n",
            "('pre3', 'in2', 'post_none')\n",
            "('pre3', 'in3', 'post1')\n",
            "('pre3', 'in3', 'post2')\n",
            "('pre3', 'in3', 'post_none')\n",
            "('pre3', 'in_none', 'post1')\n",
            "('pre3', 'in_none', 'post2')\n",
            "('pre3', 'in_none', 'post_none')\n",
            "('pre4', 'in1', 'post1')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "This use of ``*`` has resulted in matrix multiplication.\n",
            "Using ``*`` for matrix multiplication has been deprecated since CVXPY 1.1.\n",
            "    Use ``*`` for matrix-scalar and vector-scalar multiplication.\n",
            "    Use ``@`` for matrix-matrix and matrix-vector multiplication.\n",
            "    Use ``multiply`` for elementwise multiplication.\n",
            "This code path has been hit 1 times so far.\n",
            "\n",
            "\n",
            "This use of ``*`` has resulted in matrix multiplication.\n",
            "Using ``*`` for matrix multiplication has been deprecated since CVXPY 1.1.\n",
            "    Use ``*`` for matrix-scalar and vector-scalar multiplication.\n",
            "    Use ``@`` for matrix-matrix and matrix-vector multiplication.\n",
            "    Use ``multiply`` for elementwise multiplication.\n",
            "This code path has been hit 2 times so far.\n",
            "\n",
            "\n",
            "This use of ``*`` has resulted in matrix multiplication.\n",
            "Using ``*`` for matrix multiplication has been deprecated since CVXPY 1.1.\n",
            "    Use ``*`` for matrix-scalar and vector-scalar multiplication.\n",
            "    Use ``@`` for matrix-matrix and matrix-vector multiplication.\n",
            "    Use ``multiply`` for elementwise multiplication.\n",
            "This code path has been hit 3 times so far.\n",
            "\n",
            "\n",
            "This use of ``*`` has resulted in matrix multiplication.\n",
            "Using ``*`` for matrix multiplication has been deprecated since CVXPY 1.1.\n",
            "    Use ``*`` for matrix-scalar and vector-scalar multiplication.\n",
            "    Use ``@`` for matrix-matrix and matrix-vector multiplication.\n",
            "    Use ``multiply`` for elementwise multiplication.\n",
            "This code path has been hit 4 times so far.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimized Preprocessing: Objective converged to 0.011942\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "This use of ``*`` has resulted in matrix multiplication.\n",
            "Using ``*`` for matrix multiplication has been deprecated since CVXPY 1.1.\n",
            "    Use ``*`` for matrix-scalar and vector-scalar multiplication.\n",
            "    Use ``@`` for matrix-matrix and matrix-vector multiplication.\n",
            "    Use ``multiply`` for elementwise multiplication.\n",
            "This code path has been hit 5 times so far.\n",
            "\n",
            "\n",
            "This use of ``*`` has resulted in matrix multiplication.\n",
            "Using ``*`` for matrix multiplication has been deprecated since CVXPY 1.1.\n",
            "    Use ``*`` for matrix-scalar and vector-scalar multiplication.\n",
            "    Use ``@`` for matrix-matrix and matrix-vector multiplication.\n",
            "    Use ``multiply`` for elementwise multiplication.\n",
            "This code path has been hit 6 times so far.\n",
            "\n",
            "\n",
            "This use of ``*`` has resulted in matrix multiplication.\n",
            "Using ``*`` for matrix multiplication has been deprecated since CVXPY 1.1.\n",
            "    Use ``*`` for matrix-scalar and vector-scalar multiplication.\n",
            "    Use ``@`` for matrix-matrix and matrix-vector multiplication.\n",
            "    Use ``multiply`` for elementwise multiplication.\n",
            "This code path has been hit 7 times so far.\n",
            "\n",
            "\n",
            "This use of ``*`` has resulted in matrix multiplication.\n",
            "Using ``*`` for matrix multiplication has been deprecated since CVXPY 1.1.\n",
            "    Use ``*`` for matrix-scalar and vector-scalar multiplication.\n",
            "    Use ``@`` for matrix-matrix and matrix-vector multiplication.\n",
            "    Use ``multiply`` for elementwise multiplication.\n",
            "This code path has been hit 8 times so far.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimized Preprocessing: Objective converged to 0.013950\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "This use of ``*`` has resulted in matrix multiplication.\n",
            "Using ``*`` for matrix multiplication has been deprecated since CVXPY 1.1.\n",
            "    Use ``*`` for matrix-scalar and vector-scalar multiplication.\n",
            "    Use ``@`` for matrix-matrix and matrix-vector multiplication.\n",
            "    Use ``multiply`` for elementwise multiplication.\n",
            "This code path has been hit 9 times so far.\n",
            "\n",
            "\n",
            "This use of ``*`` has resulted in matrix multiplication.\n",
            "Using ``*`` for matrix multiplication has been deprecated since CVXPY 1.1.\n",
            "    Use ``*`` for matrix-scalar and vector-scalar multiplication.\n",
            "    Use ``@`` for matrix-matrix and matrix-vector multiplication.\n",
            "    Use ``multiply`` for elementwise multiplication.\n",
            "This code path has been hit 10 times so far.\n",
            "\n",
            "\n",
            "This use of ``*`` has resulted in matrix multiplication.\n",
            "Using ``*`` for matrix multiplication has been deprecated since CVXPY 1.1.\n",
            "    Use ``*`` for matrix-scalar and vector-scalar multiplication.\n",
            "    Use ``@`` for matrix-matrix and matrix-vector multiplication.\n",
            "    Use ``multiply`` for elementwise multiplication.\n",
            "This code path has been hit 11 times so far.\n",
            "\n",
            "\n",
            "This use of ``*`` has resulted in matrix multiplication.\n",
            "Using ``*`` for matrix multiplication has been deprecated since CVXPY 1.1.\n",
            "    Use ``*`` for matrix-scalar and vector-scalar multiplication.\n",
            "    Use ``@`` for matrix-matrix and matrix-vector multiplication.\n",
            "    Use ``multiply`` for elementwise multiplication.\n",
            "This code path has been hit 12 times so far.\n",
            "\n",
            "Solution may be inaccurate. Try another solver, adjusting the solver settings, or solve with verbose=True for more information.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimized Preprocessing: Objective converged to 0.011243\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('pre4', 'in1', 'post2')\n",
            "('pre4', 'in1', 'post_none')\n",
            "('pre4', 'in2', 'post1')\n",
            "iteration: 1, error: 0.23338337655247715, fairness violation: 0.0049999295798760605, violated group size: 0.25235430599153813\n",
            "iteration: 2, error: 0.23338337655247715, fairness violation: 0.0049999295798760605, violated group size: 0.25235430599153813\n",
            "iteration: 3, error: 0.23338337655247715, fairness violation: 0.0049999295798760605, violated group size: 0.25235430599153813\n",
            "iteration: 4, error: 0.23338337655247715, fairness violation: 0.0049999295798760605, violated group size: 0.25235430599153813\n",
            "iteration: 5, error: 0.23338337655247715, fairness violation: 0.0049999295798760605, violated group size: 0.25235430599153813\n",
            "('pre4', 'in2', 'post2')\n",
            "('pre4', 'in2', 'post_none')\n",
            "('pre4', 'in3', 'post1')\n",
            "('pre4', 'in3', 'post2')\n",
            "('pre4', 'in3', 'post_none')\n",
            "('pre4', 'in_none', 'post1')\n",
            "('pre4', 'in_none', 'post2')\n",
            "('pre4', 'in_none', 'post_none')\n",
            "('pre_none', 'in1', 'post1')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('pre_none', 'in1', 'post2')\n",
            "('pre_none', 'in1', 'post_none')\n",
            "('pre_none', 'in2', 'post1')\n",
            "iteration: 1, error: 0.20349392657294937, fairness violation: 0.015291740163445939, violated group size: 0.2908420908966835\n",
            "iteration: 2, error: 0.22512624539374915, fairness violation: 0.0076458700817229695, violated group size: 0.2908420908966835\n",
            "iteration: 3, error: 0.23233701833401577, fairness violation: 0.005097246721148648, violated group size: 0.46239934488876755\n",
            "iteration: 4, error: 0.23594240480414905, fairness violation: 0.0038229350408614847, violated group size: 0.2908420908966835\n",
            "iteration: 5, error: 0.238105636686229, fairness violation: 0.0030583480326891897, violated group size: 0.46239934488876755\n",
            "iteration: 6, error: 0.2395477912742823, fairness violation: 0.0025486233605743237, violated group size: 0.2908420908966835\n",
            "iteration: 7, error: 0.24057790169432042, fairness violation: 0.002184534309063706, violated group size: 0.2908420908966835\n",
            "iteration: 8, error: 0.24135048450934898, fairness violation: 0.0019114675204307432, violated group size: 0.2908420908966835\n",
            "iteration: 9, error: 0.24195138225437118, fairness violation: 0.0016990822403828833, violated group size: 0.46239934488876755\n",
            "iteration: 10, error: 0.242432100450389, fairness violation: 0.0015291740163445944, violated group size: 0.2908420908966835\n",
            "iteration: 11, error: 0.2428254153380399, fairness violation: 0.0013901581966769042, violated group size: 0.46239934488876755\n",
            "iteration: 12, error: 0.24204995223147263, fairness violation: 0.0014719304229692716, violated group size: 0.2908420908966835\n",
            "iteration: 13, error: 0.24139379114130036, fairness violation: 0.0015411223067551211, violated group size: 0.2908420908966835\n",
            "iteration: 14, error: 0.2408313673497241, fairness violation: 0.0016004296357144216, violated group size: 0.46239934488876755\n",
            "iteration: 15, error: 0.24034393339702473, fairness violation: 0.001651829320812481, violated group size: 0.2908420908966835\n",
            "iteration: 16, error: 0.23991742868841273, fairness violation: 0.001696804045273283, violated group size: 0.2908420908966835\n",
            "iteration: 17, error: 0.23942870446936795, fairness violation: 0.0017488872330638488, violated group size: 0.46239934488876755\n",
            "iteration: 18, error: 0.2389942829413281, fairness violation: 0.001795183399988796, violated group size: 0.2908420908966835\n",
            "iteration: 19, error: 0.23860558999518722, fairness violation: 0.0018366062861848015, violated group size: 0.46239934488876755\n",
            "iteration: 20, error: 0.23732769209772078, fairness violation: 0.0021663626229307285, violated group size: 0.46239934488876755\n",
            "iteration: 21, error: 0.2361714987619178, fairness violation: 0.0024647135942722814, violated group size: 0.46239934488876755\n",
            "iteration: 22, error: 0.23512041391118776, fairness violation: 0.00273594175003733, violated group size: 0.46239934488876755\n",
            "iteration: 23, error: 0.23416072774312993, fairness violation: 0.0029835848487793307, violated group size: 0.2908420908966835\n",
            "iteration: 24, error: 0.23328101542241025, fairness violation: 0.0032105910226261644, violated group size: 0.2908420908966835\n",
            "iteration: 25, error: 0.23247168008734814, fairness violation: 0.003419436702565252, violated group size: 0.46239934488876755\n",
            "iteration: 26, error: 0.23172460131652162, fairness violation: 0.003612217330201334, violated group size: 0.46239934488876755\n",
            "iteration: 27, error: 0.23103286171390444, fairness violation: 0.0037907179113458515, violated group size: 0.2908420908966835\n",
            "iteration: 28, error: 0.23039053208290278, fairness violation: 0.003956468450980047, violated group size: 0.2908420908966835\n",
            "iteration: 29, error: 0.2297925010471426, fairness violation: 0.004110787918915334, violated group size: 0.46239934488876755\n",
            "iteration: 30, error: 0.22923433874709978, fairness violation: 0.0042548194223216, violated group size: 0.2908420908966835\n",
            "iteration: 31, error: 0.22871218691802747, fairness violation: 0.004389558570669399, violated group size: 0.46239934488876755\n",
            "iteration: 32, error: 0.22822266957827214, fairness violation: 0.00451587652224546, violated group size: 0.2908420908966835\n",
            "iteration: 33, error: 0.22772559772696255, fairness violation: 0.004656895708251654, violated group size: 0.2908420908966835\n",
            "iteration: 34, error: 0.22725776539631823, fairness violation: 0.004789619648022191, violated group size: 0.2908420908966835\n",
            "iteration: 35, error: 0.22681666634171072, fairness violation: 0.0049147593626629816, violated group size: 0.2908420908966835\n",
            "iteration: 36, error: 0.22640007279013694, fairness violation: 0.005032946870934841, violated group size: 0.46239934488876755\n",
            "iteration: 37, error: 0.22603919601326453, fairness violation: 0.00512480595607453, violated group size: 0.46239934488876755\n",
            "iteration: 38, error: 0.22569731275096436, fairness violation: 0.005211830352522657, violated group size: 0.2908420908966835\n",
            "iteration: 39, error: 0.22537296196365394, fairness violation: 0.005294391959409343, violated group size: 0.46239934488876755\n",
            "iteration: 40, error: 0.22552886583867887, fairness violation: 0.005226587616366929, violated group size: 0.2908420908966835\n",
            "iteration: 41, error: 0.22572376808796069, fairness violation: 0.0051569495016307925, violated group size: 0.2908420908966835\n",
            "iteration: 42, error: 0.2259093892777529, fairness violation: 0.005090627487596378, violated group size: 0.2908420908966835\n",
            "iteration: 43, error: 0.22608637692383382, fairness violation: 0.0050273902184007685, violated group size: 0.2908420908966835\n",
            "iteration: 44, error: 0.2262553196769111, fairness violation: 0.0049670273705322366, violated group size: 0.2908420908966835\n",
            "iteration: 45, error: 0.22641675386318486, fairness violation: 0.004909347315902305, violated group size: 0.46239934488876755\n",
            "iteration: 46, error: 0.22657116917179465, fairness violation: 0.004854175089734544, violated group size: 0.2908420908966835\n",
            "iteration: 47, error: 0.2267190136162082, fairness violation: 0.004801350617871795, violated group size: 0.2908420908966835\n",
            "iteration: 48, error: 0.2268606978754379, fairness violation: 0.004750727165669994, violated group size: 0.2908420908966835\n",
            "iteration: 49, error: 0.2269965991036786, fairness violation: 0.004702169976823368, violated group size: 0.46239934488876755\n",
            "('pre_none', 'in2', 'post2')\n",
            "('pre_none', 'in2', 'post_none')\n",
            "('pre_none', 'in3', 'post1')\n",
            "('pre_none', 'in3', 'post2')\n",
            "('pre_none', 'in3', 'post_none')\n",
            "('pre_none', 'in_none', 'post1')\n",
            "('pre_none', 'in_none', 'post2')\n",
            "('pre_none', 'in_none', 'post_none')\n"
          ]
        }
      ],
      "source": [
        "bc,bs = test_preprocessing_combinations(dataset_orig_train, dataset_orig_valid, dataset_orig_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JS10BFqX1id7",
        "outputId": "d15fc733-068d-4b4e-d2e2-9fc4f9579ec6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('pre1', 'in1', 'post_none')"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "bc"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(str(preprocessing_algos[bc[0]]))\n",
        "print(str(in_processing_algos[bc[1]]))\n",
        "print(str(post_processing_algos[bc[2]]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yhLVpA5HS0Xy",
        "outputId": "567602a9-13ca-4e0c-f091-2f597cfb1a19"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<function reweighing at 0x7fb1c440c040>\n",
            "<function exp_grad_red_in_proc at 0x7fb1c440cee0>\n",
            "<function post_no_fn at 0x7fb1c440d360>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zTo8vs1I1id7",
        "outputId": "6d663188-8036-4e52-fd29-385801081871"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.012422874352058374"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "bs"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eJYLNRUtTSko"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}